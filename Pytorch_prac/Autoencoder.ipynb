{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140137b6-2a21-4d73-bcc7-7f2034826c6e",
   "metadata": {},
   "source": [
    "# AutoEncoder for feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d730f-185b-4cb0-a2fc-7f24d4c6c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torchvision\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2018cd-218b-4674-9c52-86b7106200f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b55622-83f8-4e48-b076-c9811eb42085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad67b38-d9de-48e2-9248-197a081a0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking current directory\n",
    "directory = os.getcwd()\n",
    "print(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19910ba1-15a6-4313-84db-a4aa431b1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "## enviroinment setting\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46e702-bcfd-4895-bcad-73dbed7b57a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61209976-fe08-4665-992a-7c345d8d28a4",
   "metadata": {},
   "source": [
    "## Dataset Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df74fbe8-40d6-416c-9cd8-444af9f304ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation for fNIRS\n",
    "data_transforms_1 = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# Data Transformation\n",
    "data_transforms_2 = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45b7d5-51ac-4ad1-80fa-186c8d0a67e3",
   "metadata": {},
   "source": [
    "* Input dataset shapes:\n",
    "> fNIRS: 682x539 \\\n",
    "> VFT: 720x720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98df3a-0797-4adb-988b-1c7560d5f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the food image data\n",
    "\n",
    "data_train = datasets.ImageFolder(root = 'E:/RESEARCH/BRAIN/research_data/FNIRS/VFT_3CLASS/train', transform = data_transforms_1) ## fNIRS\n",
    "# data_train = datasets.ImageFolder(root = 'E:/RESEARCH/BRAIN/research_data/VFT_3CLASS', transform = data_transforms_2) ##VFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32600793-7701-4e5e-93cf-150aed24a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## arguments setting for hyperparameter tuning\n",
    "class Args:\n",
    "    # arugments\n",
    "    epochs=150\n",
    "    bs=32\n",
    "    lr=0.0001\n",
    "    momentum=0.9\n",
    "    \n",
    "    num_channels=3\n",
    "    num_classes=2\n",
    "    verbose='store_true'\n",
    "    seed=712002\n",
    "\n",
    "args = Args()    \n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570b0ee-7aba-40b8-a8a2-f3577af825e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the overall dataset into train and test dataset\n",
    "train_size = int(0.8 * len(data_train))\n",
    "test_size = len(data_train)-train_size\n",
    "print('Training dataset size is:', train_size, '/ Test dataset size is:', test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08daab1-d1e0-4078-aed7-40a93ab0fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split for model training\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(data_train, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560dc718-4143-4ccd-b669-a75f95790fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff4426-4c64-4cf7-8658-d904cb787451",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74c8ce-9db3-494c-adff-a57ba4b7f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].size()\n",
    "# images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eb3084-6762-48c0-8310-924555345583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_reshape = images.reshape([-1, 512*512])\n",
    "# image_reshape.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85362f03-5ca2-4424-8c88-0820655c138d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5ab38e5-cfe0-4b02-b414-0d2f85938d74",
   "metadata": {},
   "source": [
    "* Image check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe0b1e-52c2-462f-8dc5-39f11ab439fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543c883-1d32-41dd-a1fe-3e32771d8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_np = images.numpy() # convert images to numpy for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9e9fe-0125-4dd6-845d-b66f31e5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(5):\n",
    "    ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images_np[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414efd2-ec4d-46c3-9b90-37d6e58535ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112155d-d54d-4a82-b989-77a1d677977b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7c7da6f-760e-4d80-9786-6529b74926b5",
   "metadata": {},
   "source": [
    "## Autoencoder Model with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c400b10-6d60-40ae-8415-6ca3d5515e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Convolutional Autoencoder for fNIRS brainimage dataset\n",
    "# class convAutoencoder_1(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(convAutoencoder_1, self).__init__()\n",
    "        \n",
    "#         # Encoder\n",
    "#         self.cnn_encoder = nn.Sequential(\n",
    "#             nn.Linear(512*512, 2048),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ReLU(2048, 128)\n",
    "#         )\n",
    "\n",
    "#         # Decoder\n",
    "#         self.cnn_decoder = nn.Sequential(\n",
    "#             nn.Linear(128, 2048),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ReLU(2048, 512*512)\n",
    "#             nn.sigmoid\n",
    "#         )\n",
    "            \n",
    "            \n",
    "#     def forward(self, x):\n",
    "#         encoder_out = self.cnn_encoder(x)\n",
    "#         decoder_out = self.cnn_decoder(encoder_out)\n",
    "#         return out\n",
    "    \n",
    "#     def get_hidden(self,x):\n",
    "#         return self.cnn_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf357a0-d86f-4923-b882-d74d44724496",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Autoencoder for VFT Vocal recording dataset\n",
    "class convAutoencoder_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convAutoencoder_1, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(16, 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.cnn_decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2, 16, kernel_size = 2, stride = 2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size = 2, stride = 2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "            \n",
    "            \n",
    "    def forward(self, x):\n",
    "        encoder_out = self.cnn_encoder(x)\n",
    "        decoder_out = self.cnn_decoder(encoder_out)\n",
    "        return decoder_out\n",
    "    \n",
    "    def get_hidden(self,x):\n",
    "        return self.cnn_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fc453-fa98-4cd2-b05f-8e6d6a3576bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Autoencoder for VFT Vocal recording dataset\n",
    "class convAutoencoder_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convAutoencoder_2, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(16, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.cnn_decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4, 16, kernel_size = 2, stride = 2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size = 2, stride = 2, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "            \n",
    "            \n",
    "    def forward(self, x):\n",
    "        encoder_out = self.cnn_encoder(x)\n",
    "        decoder_out = self.cnn_decoder(encoder_out)\n",
    "        return decoder_out\n",
    "    \n",
    "    def get_hidden(self,x):\n",
    "        return self.cnn_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eea09b-c0bb-4fd2-b08a-505355e6a411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d291889-a299-4d73-878b-78f1052bbf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f81a7-2cd8-4daa-b518-e754c3e2fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "# model = convAutoencoder_1().to(DEVICE)\n",
    "model = convAutoencoder_1().to(DEVICE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = args.lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107637f-a736-470a-b7fe-29747f9e86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, args.epochs+1):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*image.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a052d2e-ead9-4fda-a93e-b477df901952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6ed32-6576-41ac-ae92-1d86ec5c0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    hidden_features = []\n",
    "    for image, label in test_loader:\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        hidden = model.cnn_encoder(image)\n",
    "        hidden_features.append(hidden)\n",
    "    hidden_features = torch.cat(hidden_features, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474eabb-c7b8-4702-a8a8-802ce8a0e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracted hidden feature shape is {} by {} vector with {} dimensionality\".format(hidden_features.shape[2],hidden_features.shape[3], hidden_features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e51654-e79a-407c-a52b-c43e0c616f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_features[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48045719-2b1f-41ea-9f24-2747458047c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991268bc-a1a3-4fd2-8042-c24fb47148e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d919bf-e981-4f9f-8951-27936a54b929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80fe61-5f81-4912-ae16-08a9f4bbc1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8bb1e1-0e57-4e9e-bb61-3cd6bce62663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577c739-c5a7-4bcf-93e9-a04c37ae3070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae739fc-1033-485f-a126-7c9ebaa39564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c410dc0-1176-44fa-a369-8621a88dd9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93b170-3c31-479a-95c4-00725e920a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a70a5-cfb8-4d5e-9804-4c7813d29862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f62e25-74d1-41e0-9a56-c740e2aed18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2fd8f2-f23f-42f0-b309-e0e7668ad194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
