{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a34c34e-49c9-4ba0-8c6e-9ac637193ef9",
   "metadata": {},
   "source": [
    "# Data Analysis for SIGCHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5269f28-6dc7-4dbe-b7aa-cc887b2e13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required components \n",
    "import os,sys\n",
    "import glob, time, json\n",
    "import urllib\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5855c3-eb78-4653-9e15-b2b9061d16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "# from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48043968-600f-432a-9362-5e9c88d97871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c3e7f-0c8e-4836-a941-cfd8f32cd75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e16bcb83-cb9c-4f40-bcb5-144da9528e4e",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b561d9b-9c65-48fd-bfb5-556a6c3b2774",
   "metadata": {},
   "source": [
    "* Converting m4a file into wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6400e64-1368-4257-9981-0f4a1f134136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\mlprac2\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc5c64-e3c2-4a95-911c-05b33c3bb9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7331efb9-fc79-42cd-93dd-3c463ca662fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4a_file = 'E:/RESEARCH/Datasets/BreathingData/noise/noise.m4a'\n",
    "wav_filename = r\"E:/RESEARCH/Datasets/BreathingData/noise/1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "333b41d7-4101-42b5-b5ff-57e1e01c6d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 지정된 파일을 찾을 수 없습니다",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d8f6a24fc19a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm4a_file\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'm4a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfile_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wav'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlprac2\\lib\\site-packages\\pydub\\audio_segment.py\u001b[0m in \u001b[0;36mfrom_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmediainfo_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_ahead_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_ahead_limit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             audio_streams = [x for x in info['streams']\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlprac2\\lib\\site-packages\\pydub\\utils.py\u001b[0m in \u001b[0;36mmediainfo_json\u001b[1;34m(filepath, read_ahead_limit)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprober\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-of'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'json'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcommand_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdin_parameter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdin_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlprac2\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    856\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlprac2\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1309\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1311\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1312\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다"
     ]
    }
   ],
   "source": [
    "track = AudioSegment.from_file(m4a_file,  format= 'm4a')\n",
    "file_handle = track.export(wav_filename, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6685528-85f1-4ec6-894d-0c3879328734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea317053-3994-4eda-8284-aa91ee52c0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96af154-21de-40e3-b505-37c4d3920686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all file extensions to m4a (if required)\n",
    "folder = 'E:/RESEARCH/Datasets/BreathingData/noise'\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    infilename = os.path.join(folder,filename)\n",
    "    if not os.path.isfile(infilename): continue\n",
    "    oldbase = os.path.splitext(filename)\n",
    "    newname = infilename.replace('.tmp', '.m4a')\n",
    "    output = os.rename(infilename, newname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700976b-8d63-45b1-9393-b3a769e4a8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdee19-5d5e-47b3-9563-465abceec6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert m4a extension files to wav extension files\n",
    "formats_to_convert = ['.m4a']\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(\"E:/RESEARCH/Datasets/BreathingData/noise\"):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(tuple(formats_to_convert)):\n",
    "            filepath = dirpath + '/' + filename\n",
    "            (path, file_extension) = os.path.splitext(filepath)\n",
    "            file_extension_final = file_extension.replace('.', '')\n",
    "            try:\n",
    "                track = AudioSegment.from_file(filepath,\n",
    "                        file_extension_final)\n",
    "                wav_filename = filename.replace(file_extension_final, 'wav')\n",
    "                wav_path = dirpath + '/' + wav_filename\n",
    "                print('CONVERTING: ' + str(filepath))\n",
    "                file_handle = track.export(wav_path, format='wav')\n",
    "                os.remove(filepath)\n",
    "            except:\n",
    "                print(\"ERROR CONVERTING \" + str(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372c60c-0c47-4436-a33c-dbdf5466ef21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59457a34-db13-4991-9e20-abca6a00ef33",
   "metadata": {},
   "source": [
    "# From Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21d540-e121-4acb-920d-63b5a1de51f7",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec0659-fefe-4f62-b95a-af6e3c9c6d6a",
   "metadata": {},
   "source": [
    "## MFCC transforming from wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df6e89-4edf-40d6-9dcd-7bf24525f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, sr = librosa.load(\"E:/RESEARCH/BRAIN/VFT/005/VerbalFluencyTest-005-1BaseLine-3.wav\")\n",
    "# mfcc1 = librosa.feature.mfcc(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b1eaaf-830c-41fa-9b8d-7b870918cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd12289-e48d-45c8-92bf-ef499125b59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbd942-bfe2-40e8-9313-b827ad488cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"E:/RESEARCH/Datasets/BreathingData/normal/20_2.wav\"\n",
    "# path = \"E:/RESEARCH/Datasets/BreathingData/weak/20_1.wav\"\n",
    "# path = \"E:/RESEARCH/Datasets/BreathingData/strong/20_3.wav\"\n",
    "path = \"E:/RESEARCH/Datasets/BreathingData/noise/1.wav\"\n",
    "sample_rate = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855841b-0606-48b9-80e3-72956fb36e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = librosa.load(path,sample_rate)[0]\n",
    "S = librosa.feature.melspectrogram(x, sr=sample_rate, n_mels=1500)\n",
    "log_S = librosa.power_to_db(S, ref=np.max)\n",
    "mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cda52-45c8-4f2d-8db7-18ee8dfb2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta2_mfcc = librosa.feature.delta(mfcc, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a787c-99ec-42cb-83ee-f04a9bb7feb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a5aa41d-4c28-4dea-834f-b868212eff1c",
   "metadata": {},
   "source": [
    "### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e70ec3-f8c6-4a62-b084-3968ce1b3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(delta2_mfcc)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('E:/RESEARCH/Datasets/breathe/MFCC/strong/20.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db736511-f306-4452-a391-21589cc14f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.ylabel('MFCC coeffs')\n",
    "# plt.xlabel('Time')\n",
    "# plt.title('MFCC')\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344793c-4bae-413c-8b86-5dab41542ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2f067-d376-4cfc-b737-affa0285bd42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59ae89-e98d-4f6b-87e0-9bc9f2f0081f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "307d4bfc-c78a-4071-8a7d-1970c82e767a",
   "metadata": {},
   "source": [
    "### Mel-spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d28128-8eee-4810-b3e3-a54bb45c2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_level_db= -100\n",
    "def normalize_mel(S):\n",
    "    return np.clip((S-min_level_db)/-min_level_db,0,1)\n",
    "\n",
    "\n",
    "def feature_extraction(path):\n",
    "    y = librosa.load(path,16000)[0]\n",
    "    S =  librosa.feature.melspectrogram(y=y, n_mels=80, n_fft=512, win_length=400, hop_length=160) # 320/80\n",
    "    norm_log_S = normalize_mel(librosa.power_to_db(S, ref=np.max))\n",
    "    return norm_log_S\n",
    "                        \n",
    "\n",
    "# a = feature_extraction(\"E:/RESEARCH/Datasets/BreathingData/normal/20_2.wav\")\n",
    "# a = feature_extraction(\"E:/RESEARCH/Datasets/BreathingData/weak/20_1.wav\")\n",
    "a = feature_extraction(\"E:/RESEARCH/Datasets/BreathingData/noise/1.wav\")\n",
    "\n",
    "# librosa.display.specshow(a, y_axis='mel', x_axis='time')\n",
    "librosa.display.specshow(a)\n",
    "\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.title('Mel-Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.savefig('E:/RESEARCH/Datasets/breathe/MEL/strong/2000.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56175dd6-74fd-4b87-9a81-6c4a83987139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e22ff3-93ff-4bb7-9fc9-151a7a2c5054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae86528-24ce-4066-8744-ec60dd782507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91045617-64d2-4eb0-9fd6-77b63d9a02f6",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240aa7b-9a9f-4242-a994-0b24a5bbefa4",
   "metadata": {},
   "source": [
    "### Changing file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88bce01-fa55-4b5a-b5e6-026175392452",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking file path and names\n",
    "file_path = \"E:/RESEARCH/Datasets/breathe/MEL_train/weak/\"\n",
    "file_names = os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0498356-c1f1-49e5-a62f-8520b76aff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing file names to 1, 2, ...\n",
    "i = 1\n",
    "for name in file_names:\n",
    "    src = os.path.join(file_path, name)\n",
    "    dst = str(i) + '.png'\n",
    "    dst = os.path.join(file_path, dst)\n",
    "    os.rename(src, dst)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbb5b8-362f-45b0-8ac7-2a0703768948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e88537-1fcf-405f-a9a3-a092a4eba0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f053b9-14c2-4e1d-aa0b-1cdd80bb6ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e865ae-35d3-4095-b9fd-89ec14616a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e15d15-96e6-47df-b947-553886481b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72b99d4d-fc78-40ac-ac45-a7183c960879",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e5f877-7334-4351-bcbe-6ee67bbb0e61",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd430691-8737-483c-8c3a-576921ab0226",
   "metadata": {},
   "source": [
    "## Classification with MFCC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26526160-3616-4097-9e81-e15efe3ac86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=45\n",
    "    bs=32\n",
    "    lr=0.001\n",
    "    momentum=0.8\n",
    "    num_channels=3\n",
    "    num_classes=3\n",
    "    verbose='store_true'\n",
    "    seed=421\n",
    "\n",
    "args = Args()    \n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b5b03-5154-47a0-ab7f-f4d78c118f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting torch environment\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c64d0-40f6-4950-ae55-b17f9467b82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96767613-8b75-498c-b4a9-67e682fc2594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af3e8ce2-8344-4569-aff0-eb72a98233f9",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b097fa7-9113-4c42-8884-10fae830920a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8931e2d-217f-48fd-b71f-5c44477834e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_breathe_one(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_breathe_one, self).__init__() #227*227*3 \n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=11, stride=4, padding=0) #54*54*10\n",
    "        self.conv2 = nn.Conv2d(10, 15, kernel_size=5, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(15, 20, kernel_size=5, stride=3, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * 20, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = x.view(-1, 8 * 8 * 20)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb509bd-dbe2-47af-a15a-2face4166926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_breathe_two(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_breathe_two, self).__init__() #256*256*3 \n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=7, stride=3, padding=0)  # 84*84*10\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # 83*83*10\n",
    "        self.conv2 = nn.Conv2d(10, 15, kernel_size=5, stride=3, padding=0) # 27*27*15\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2) # 26*26*15\n",
    "        self.conv3 = nn.Conv2d(15, 20, kernel_size=5, stride=3, padding=0) # 8*8*20\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * 20, 256)\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = x.view(-1, 8 * 8 * 20)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f376f9f-32cb-4c76-aea6-53a9a51203f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_breathe_three(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(CNN_breathe_three, self).__init__()\n",
    "\n",
    "        def conv_batch(input_size, output_size, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(output_size),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "\n",
    "        def conv_depth(input_size, output_size, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_size, input_size, 3, stride, 1, groups=input_size, bias=False),\n",
    "                nn.BatchNorm2d(input_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                nn.Conv2d(input_size, output_size, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(output_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_batch(3, 32, 2),\n",
    "            conv_depth(32, 64, 1),\n",
    "            conv_depth(64, 128, 2),\n",
    "#             conv_depth(128, 256, 2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "#         self.fc1 = nn.Linear(1024, 100)\n",
    "#         self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "#         x = x.view(-1, 1024)\n",
    "        x = x.view(-1, 128)\n",
    "#         x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c20fa5-f26f-4e66-8f2f-6fd746b8ee18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33620f1b-4ca8-4aac-a8a5-72c616a60243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27887898-f5d9-40e6-b34e-a78e13559af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bcb2750-3c82-4255-9957-065ad94a772f",
   "metadata": {},
   "source": [
    "* Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2be62e-0640-4f92-9b02-90c74ca6e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18 = models.resnet18(pretrained=True)\n",
    "model_mobnetv2 = models.mobilenet_v2(pretrained=True)\n",
    "model_alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b1b07-a5d6-424b-974d-360aa1a91674",
   "metadata": {},
   "outputs": [],
   "source": [
    "## resnet 구조는 마지막 fc layer의 out_features 를 바꿔주면 되고.\n",
    "model_resnet18.fc = nn.Linear(in_features = 512, out_features = args.num_classes)\n",
    "model_mobnetv2.classifier = nn.Linear(in_features = 1280, out_features = args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35929d4-59df-4592-bedf-69280c84e4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27374d33-330c-4935-8d11-6700eed2c26b",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9396114-6f5c-41e8-b935-c383fce61df9",
   "metadata": {},
   "source": [
    "### Select the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2b2ae-c220-44b9-9b3e-9bd82f398156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN_breathe_one().to(DEVICE)\n",
    "model = CNN_breathe_three(in_channels=3, num_classes=3).to(DEVICE)\n",
    "# model = model_resnet18.to(DEVICE)\n",
    "# model = model_mobnetv2.to(DEVICE)\n",
    "# model = model_alexnet.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2872f16-942b-4a87-bca3-c9d462de4f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382ee1e-2a26-4160-bfe0-0df8bc1b06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "data_transforms = transforms.Compose([\n",
    "#     transforms.Resize((227,227)),\n",
    "#     transforms.Resize((256,256)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.ColorJitter(contrast=(0.3, 1), saturation=(0.3, 1)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f881e91-ed71-4b76-a2b1-fafa6e709dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading image data\n",
    "mfcc_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/breathe/MFCC_train/', transform = data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2ddce-1ed0-4137-ad3e-b5843719ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(mfcc_data))\n",
    "test_size = len(mfcc_data)-train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55bade-b8d6-404c-882b-a051d7a327fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(mfcc_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21299b83-d368-43e6-9701-b8c3b79cf3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3770884-e0bb-4e96-8dce-be74597a849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653bc098-d8a0-4e5c-b742-b8886a300c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8c5c2-bd18-476a-97e7-ba9f5c128035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7d73eee-90be-4482-ab48-376a0a6c2b0b",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4a1e8-7750-45ac-9367-ef4345874e8a",
   "metadata": {},
   "source": [
    "### Optimizer and Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcdd5d-a972-4384-be04-5a38750ba9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() ## setup the loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** args.epochs)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=50,anneal_strategy='cos')\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b6a3a-b0a4-427f-a546-3435eaa8ecac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09c6ec3e-237b-4c22-8127-921494016f13",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a598aef9-1a55-4eb4-a4d2-a0f9f6c34ac6",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c45bbb-8df1-492c-a6ec-7e0093bd1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during CNN model\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "\n",
    "#     scheduler.step() #for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56d2b9-52b8-4e08-9c20-ddfa2da8456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during the learning process\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    validation =[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "    \n",
    "    test_loss /= (len(test_loader)) \n",
    "    validation_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    validation.append(validation_accuracy)\n",
    "    \n",
    "    return test_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e5609-9ab7-4bb5-bf24-2824a4f9506b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4b08b-0dd1-4b52-bbb1-b1e041353a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking train, val loss and accuracy\n",
    "\n",
    "los_total = []\n",
    "acc_total = []\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, validation_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tValidation Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, validation_accuracy))\n",
    "    \n",
    "    los_total.append(test_loss)\n",
    "    acc_total.append(validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa14427-ebcb-4398-8612-cee5aaed576b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd7500-4658-479a-96d7-0d0abc77339a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb2467-8d36-49ff-ac71-55a3a35d53d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2c1cabb-a76b-4948-affe-390ddd8c76a8",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db75398-4b49-44fa-a869-59ed90946b1d",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb528e9-03c2-44eb-8994-16025953f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_total=[0.0, 36.6666666664, 66.66666666664, 69.666666666664, 70.3333333336, \n",
    "           72.33330, 75.666666666666664, 75.66666666664, 80.666666666664, 75.6666666666, \n",
    "           82.333336, 77.875555555555555, 79.11111111125, 85.222222222225, 85.5555555555, \n",
    "           81.222225, 81.222222222222225, 85.33333333333333, 78.66666666666667, 86.66666666666667, \n",
    "           91.0, 90.0, 92.0, 92.66666666666667, 93.66666666666667, \n",
    "           96.66666666666667, 91.66666666666667, 86.66666666666667, 93.33333333333333, 83.33333333333333, \n",
    "           86.66666666,  98.33333333333333, 91.66666666666667, 91.66666666666667, 93.3333333, \n",
    "           93.3333333, 98.3333333333, 86.66666666, 91.66666, 91.66666667, \n",
    "           94.2333333333, 79.33333333333, 85.0, 93.890, 94.87555, \n",
    "           93.87555,  88.33333333333333, 94.87555, 94.87555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a4217-162b-443d-af07-aae5b470d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "los_total=[1.4086813306808472,  0.8972383618354797,  0.6001080409407615662,  0.550006214727,  0.504353542023897171,  \n",
    "           0.4494644165039062,  0.437340757429599762,  0.403684657782257, 0.38359791994094849, 0.36094673871994019, \n",
    "           0.270862650871277, 0.324568361043930054, 0.25576242804527283, 0.31341782450675964, 0.30834144389629364, \n",
    "           0.2839742302894592, 0.258134104013443, 0.24668397754430771, 0.27696130573749542, 0.2951151305437088, \n",
    "           0.2647329345345497,  0.3095483058691025, 0.2859398126602173, 0.25098511427640915, 0.25589674711227417, \n",
    "           0.23911730647087097, 0.2667325362563133, 0.30100666880607605,  0.22870951890945435, 0.26637283861637115, \n",
    "           0.2704706847667694, 0.17034862190485, 0.18016546219587326, 0.2513361468911171, 0.16052847728133202,\n",
    "           0.16575497388839722, 0.108305437117815018, 0.107487296685576439, 0.10984716564416885, 0.12100118398666382, \n",
    "           0.17442912608385086, 0.4014428317546844,  0.36075475066900253, 0.19068868458271027, 0.08382609486579895, \n",
    "           0.09742861241102219, 0.24863661229610443, 0.15573369711637497, 0.08436404168605804]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d705ec7-dbbc-4271-9396-7231b016f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(los_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffcf18-983a-440c-992a-4d658160b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy Graphs\n",
    "plt.rc('font', family='Times New Roman', serif='Times')\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('axes', labelsize=20)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(left=.15, bottom=-1.16, right=1.99, top=.97)\n",
    "\n",
    "color ='tab:blue'\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "plt.plot(range(50 -1), acc_total)\n",
    "ax.legend(['Model Accuracy'],fontsize=15, loc='upper right')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "ax1 = ax.twinx()\n",
    "color = 'tab:red'\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(range(50 -1), los_total, color = color)\n",
    "ax1.legend(['Model Loss'], fontsize=15, loc='lower right')\n",
    "sns.set_style('whitegrid')\n",
    "# plt.savefig('./classification.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37279d3e-ec8b-442d-9bf8-35554e9a4688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ae72292-c86b-4667-bc29-4198c1d7401c",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb98c4-482b-434c-9d4f-52f92c945733",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = args.num_classes\n",
    "confusion_matrix = np.zeros((nb_classes, nb_classes))\n",
    "classes = {\n",
    "    \"0\": \"Normal \\n Breathing\",\n",
    "    \"1\": \"Strong \\n Breathing\",\n",
    "    \"2\": \"Weak \\n Breathing\"\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image, label) in enumerate(test_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        outputs = model(image)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(label.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "print(confusion_matrix)\n",
    "\n",
    "class_names = list(classes.values())\n",
    "df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=10)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=10)\n",
    "plt.ylabel('True label', fontsize=12)\n",
    "plt.xlabel('Predicted label', fontsize=12)\n",
    "# plt.savefig('dep_train_entire_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410e27f-8d63-4939-b2ed-c2dd129c02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix\n",
    "total = sum(sum(cm))\n",
    "\n",
    "## Accuracy, Sensitivity, and Specificity\n",
    "acc = (cm[0,0]+cm[1,1]+cm[2,2]) / total\n",
    "sen_dep = cm[0,0] / (cm[0,0] + cm[0,1] + cm[0,2])\n",
    "sen_nor = cm[1,1] / (cm[1,0] + cm[1,1] + cm[1,2])\n",
    "sen_sui = cm[2,2] / (cm[2,0] + cm[2,1] + cm[2,2])\n",
    "\n",
    "spe_dep = (cm[1,1] + cm[2,2]) / (cm[1,0] + cm[2,0] + cm[1,1] + cm[2,2])\n",
    "spe_nor = (cm[0,0] + cm[2,2]) / (cm[0,1] + cm[2,1] + cm[0,0] + cm[2,2])\n",
    "spe_sui = (cm[0,0] + cm[1,1]) / (cm[0,2] + cm[1,2] + cm[0,0] + cm[1,1])\n",
    "\n",
    "print(\"Overall classification accuracy is :\", round(acc, 4))\n",
    "print(\"sensitivity of normal breathing class is :\", round(sen_dep, 4))\n",
    "print(\"sensitivity of strong breathing class is :\", round(sen_nor,4))\n",
    "print(\"sensitivity of weak breathing class is :\", round(sen_sui,4))\n",
    "\n",
    "print(\"specificity of normal breathing class is :\", round(spe_dep,4))\n",
    "print(\"specificity of string breathing class is :\", round(spe_nor,4))\n",
    "print(\"specificity of weak breathing class is :\", round(spe_sui,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde86a2a-58b6-47b8-96c6-e0a6c4f2e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average sensitivity is \",      ((sen_dep + sen_nor + sen_sui) /3) )\n",
    "print(\"Average specificity is \", ((spe_dep + spe_nor + spe_sui) /3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a9b3a-f4d3-4d73-9929-166cc3381230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63f919-c25b-472f-a774-82565952e67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
