{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecd52dc-3d90-4c92-b374-6c8b2f09d9af",
   "metadata": {},
   "source": [
    "# Video Classification with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db17dcd-7ea2-493b-aec9-d8472ebb1abd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. PyTorch 3D RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f20188-ce06-43ac-b19b-b53a8c56d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required components \n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defd4af-ebbe-4344-b443-fa29b971967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing remaining components\n",
    "import json\n",
    "import urllib\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15474af7-9d3f-4ce1-b580-c429de5324e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the `slow_r50` pretrained model - for our video classification model training \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3e29f-9109-4efd-a9bb-4dae93f4d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE setup - Set the model to eval mode and move to desired device.\n",
    "# Set to GPU or CPU\n",
    "\n",
    "device = \"cpu\"\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553cb5a-f2bc-4bc1-bbc1-41b60f80ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the id to label mapping for the Kinetics 400 dataset on which the torch hub models were trained. \n",
    "#This will be used to get the category label names from the predicted class ids.\n",
    "\n",
    "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "json_filename = \"kinetics_classnames.json\"\n",
    "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "except: urllib.request.urlretrieve(json_url, json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2a8a6-6198-4f3e-96ce-24e7a9797c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_filename, \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6d027-f76a-4bab-a084-dd3890d2fd5f",
   "metadata": {},
   "source": [
    "* Input Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba436b4-032f-4091-a960-942d82439135",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 8\n",
    "sampling_rate = 8\n",
    "frames_per_second = 30\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f574d8-8854-4a21-a450-a0b861600692",
   "metadata": {},
   "source": [
    "* Loading video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b530da-f207-465e-9d03-1fe026c3c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
    "video_path = 'archery.mp4'\n",
    "try: urllib.URLopener().retrieve(url_link, video_path)\n",
    "except: urllib.request.urlretrieve(url_link, video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df94b3c-c1d8-4f41-9fe4-d59ba2849e1b",
   "metadata": {},
   "source": [
    "* Load the video and transform into input format (for the model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60431d9e-7340-4821-82dd-9a78eefe41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the duration of the clip to load by specifying the start and end duration\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "\n",
    "# Initialize an EncodedVideo helper class and load the video\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)\n",
    "\n",
    "# Move the inputs to the desired device\n",
    "inputs = video_data[\"video\"]\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6a356-4d20-47c8-890c-d463dae30c90",
   "metadata": {},
   "source": [
    "* Predictions on Video Clip. Output come with top 5 predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295d9e6-3ba8-419f-be3c-c4fb90a85ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input clip through the model\n",
    "preds = model(inputs[None, ...])\n",
    "\n",
    "# Get the predicted classes\n",
    "post_act = torch.nn.Softmax(dim=1)\n",
    "preds = post_act(preds)\n",
    "pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "# Map the predicted classes to the label names\n",
    "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c321ae3-74e7-4e4a-9133-fa2fe9e73a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "485cee6c-8ec0-4d67-9f9b-368eddbb534a",
   "metadata": {},
   "source": [
    "## 2. PyTorch - using pytorchvideo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e624d-bbf4-46b6-9834-3eb0e8a76102",
   "metadata": {},
   "source": [
    "https://github.com/YuxinZhaozyx/pytorch-VideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54801f60-9c9e-443c-bbb0-51ff80cdff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import PIL\n",
    "import collections\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aed5c7-2cad-4d70-906f-c5cdebe89461",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size = 2, shuffle = True)\n",
    "\n",
    "dataset = datasets.VideoDataset(\n",
    "\t\"./data/example_video_file.csv\",\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        transforms.VideoFilePathToTensor(max_len=50, fps=10, padding_mode='last'),\n",
    "        transforms.VideoRandomCrop([512, 512]),\n",
    "        transforms.VideoResize([256, 256]),\n",
    "    ])\n",
    ")\n",
    "\n",
    "for videos in data_loader:\n",
    "    print(videos.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c9e58-5cd6-4294-a126-0e76dddbc6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775806a-ef7b-492d-a5cc-be0ba9a7abbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "885fb4b4-6a3d-479e-86c7-5c3cf7c42c8e",
   "metadata": {},
   "source": [
    "## VC Autism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5f6f4-8206-4a0c-a164-f30bff3b2bbb",
   "metadata": {},
   "source": [
    "### Autism Non-Autism majore differences\n",
    "\n",
    "1. good eye contact\n",
    "2. sits relatively still"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a503c8-7a11-427c-a9ff-d09ffe0896ca",
   "metadata": {},
   "source": [
    "* image classification approach\n",
    "* Concept: train significant feature images of autism patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b1d5f5-ba52-46b4-9281-75506d5d28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ae84f-4c4b-4fa8-98f8-3fbc0904d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b4c36-f5e7-4515-b093-227aa9d1f765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f74db877-974d-4709-b879-9cffa34de45c",
   "metadata": {},
   "source": [
    "* Changing file names in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f858b-18d1-4925-9fed-42242362bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"E:/RESEARCH/Datasets/VC/autism_test/val/nonautism\"\n",
    "# file_names = os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533f26b-1577-4f38-8ab4-f726a9fe0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# for name in file_names:\n",
    "#     src = os.path.join(file_path, name)\n",
    "#     dst = str(i) + '.png'\n",
    "#     dst = os.path.join(file_path, dst)\n",
    "#     os.rename(src, dst)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1affb307-e787-4329-b3b9-8b2669bee3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354d501-2ab8-48b9-8f10-d3f98f05da57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2868d0-2c95-453a-ba08-8c935243fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=30\n",
    "    bs=6\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    \n",
    "    num_channels=3\n",
    "    num_classes=2\n",
    "    verbose='store_true'\n",
    "    seed=712002\n",
    "\n",
    "args = Args()    \n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2875c-cb83-4d33-b3b2-b6b0c993933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting torch environment\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59af08-c28f-4854-83bf-168b21899c3f",
   "metadata": {},
   "source": [
    "* Data transformation for some augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388741e-cbec-438e-8b27-0315610f66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "data_transforms = transforms.Compose([\n",
    "#     transforms.CenterCrop(1024),\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "#     transforms.ColorJitter(contrast=(0.3, 1), saturation=(0.3, 1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e287006-45e5-4069-9076-3757b618e082",
   "metadata": {},
   "source": [
    "* Setting directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ad5c2-7a7b-4001-b4b2-9892d25faa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading image data\n",
    "# data_dir = 'E:/RESEARCH/Datasets/VC/autism_test/train'\n",
    "printer_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/VC/autism_test/train', transform = data_transforms)\n",
    "# printer_data = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5710a-35c3-4750-919c-a24ec67464db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(printer_data))\n",
    "test_size = len(printer_data)-train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4877b02-a280-471f-9daa-56bc68e6754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8db0c-7369-4943-9b8b-968d9f77dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(printer_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb45770-48f8-42ab-adf7-014abcddf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e742b1-10f9-4b8d-be4f-169acc84e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af646223-dc73-4a8d-adbb-97d7e36f9129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae951300-9507-4904-b5b6-d090c88900ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_res = models.resnet18(num_classes=2, pretrained=True)\n",
    "model_eff3 = EfficientNet.from_pretrained('efficientnet-b3', num_classes=2)\n",
    "# model = model_res.to(DEVICE)\n",
    "model = model_eff3.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8ec73-0a01-4300-acca-5abe560bbae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ce396-78c5-44ee-8754-17d53168692f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8671f1d-f31d-4c6c-8294-03daf219d2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f7ffc2-40a6-45e2-927e-0aa04c92e659",
   "metadata": {},
   "source": [
    "* Model training and Accuracy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac199f7d-d56e-4bd3-850a-dc3cccfc619b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=30, anneal_strategy='cos')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6bda3-fb69-43bb-babc-409b2422ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during CNN model\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "\n",
    "    scheduler.step() #for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77eff1d-7246-4330-b87a-dd46773f574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during the learning process\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader)) \n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b88658-1cfb-4d35-acb0-739391f40c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking train, val loss and accuracy\n",
    "\n",
    "total = []\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))\n",
    "    \n",
    "    total.append((test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84e2d5-394a-46ee-bdb0-6d566a2ebeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de3ae9-4478-466d-a766-892ead5423f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dec81d-5481-4c38-8614-0ef2b54476dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ## please first init this to the path of your model parameters, e.g., './xxxxx.pth'\n",
    "    model_path = 'E:/RESEARCH/Datasets/VC/models/track1_model.pt'\n",
    "    # change this to your student id\n",
    "    your_stu_id = '2020712002'\n",
    "\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = 'cpu' if not cuda else 'cuda'\n",
    "\n",
    "    ########## Load your model #############\n",
    "\n",
    "    your_model = CNN_food(in_channels=3, num_classes=50)\n",
    "    your_model.load_state_dict(torch.load(your_model_path))\n",
    "    your_model.to(device)\n",
    "    \n",
    "    print('Model loaded')\n",
    "\n",
    "    ########## Load evaluation dataset ##########\n",
    "    transform_list = [\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor()]\n",
    "\n",
    "    # if normalization is applied in your training, you can utilize the codes below.\n",
    "    \n",
    "    if normalize:\n",
    "        transform_list.append(\n",
    "            transforms.Normalize(mean=[0.485, 0.456,0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "            )\n",
    "    \n",
    "\n",
    "\n",
    "    _transforms = transforms.Compose(transform_list)\n",
    "\n",
    "    data_folder_path = 'E:/RESEARCH/Datasets/VC/autism_test/val'\n",
    "    test_dataset = TestDataSet(root=data_folder_path, transform=_transforms)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=cuda)\n",
    "    print('Dataset loaded')\n",
    "\n",
    "    ######### evaluate ###########\n",
    "    print('Evaluating...')\n",
    "    preds_list, image_names = evaluate(model=your_model, loader=test_dataloader, device=device)\n",
    "\n",
    "    filename = your_stu_id + '.csv'\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        eval_writer = csv.writer(csvfile, delimiter=',')\n",
    "        eval_writer.writerow(['ID', 'Category'])\n",
    "        for i in range(len(preds_list)):\n",
    "            eval_writer.writerow([i, int(preds_list[i])])\n",
    "\n",
    "    print('Done!')\n",
    "    print('Results saved at : ', os.path.join(os.getcwd(), filename))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240aadf5-98ad-4ff5-8cdc-9d0b95670ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed93e45-7355-4741-8a7a-a3110531b259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c53608-debf-49c6-b6f2-b976014b1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333be454-da90-4e1e-8e40-ee1525000ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = args.lr, momentum = args.momentum)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer = optimizer,\n",
    "#                                        lr_lambda = lambda epoch:0.95 ** epoch,\n",
    "#                                        last_epoch = -1,\n",
    "#                                        verbose = False)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, \n",
    "#                                                 steps_per_epoch=10, epochs=10,anneal_strategy='linear')\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=50,anneal_strategy='cos')\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ca959-732c-417d-a068-a741f98bc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = 'E:/RESEARCH/Datasets/VC/autism_test/val'\n",
    "test_dataset = datasets.ImageFolder(root=data_folder_path, transform=data_transforms)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6d4df-cfd7-4ec5-b3d7-6688f287f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 50):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_dataloader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d45bed-ba04-48bf-a30a-3d547c88db0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce365ac-b47b-478c-95a6-d18adac52a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db5112-98ce-40ec-8f8b-5f7df92c428d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124d243-7506-4e88-b54b-091d65f40afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c895a-1c47-42ea-80b0-2c3ee73b6c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d2c19-c0fc-4926-94ee-ef2b928e3ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032fe8b6-488c-4de9-9d78-877c4b36d973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712c09e-05f4-4ce4-a08e-3bc7821eec21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ba856-ffca-4ec1-b227-94b7e100111f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22d9c4-8b34-4d50-becb-4743403e2686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b028d5c-7de4-42d9-bc1b-8dfbc7237e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
