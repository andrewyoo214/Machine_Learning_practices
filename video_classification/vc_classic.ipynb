{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fd5ae6-fddc-4a58-893a-5998feb39f8d",
   "metadata": {},
   "source": [
    "# Video Classification on CM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1da5cc-5ee8-4987-b5b4-0225c4c4884a",
   "metadata": {},
   "source": [
    "* Here we generate real-time classical music instruments information generator on classical music concert video input. \n",
    "* We basically adopt video classification method using pytorch, to classify what instrument is currently viewed on the screen. \n",
    "* Audience will receive the streaming video with information on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0c3eb-4bba-4c1c-856b-ffcf38aeed45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2f13d-b751-498b-979d-1c54d8af3c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f896f26-7f7e-4751-a302-21bd10505fc1",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fd2e4-6ec5-430b-86b0-3f5a458afdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required components \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import urllib\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c56b5c-1d80-48b1-a1d0-384dd1c944d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a75ce7-9dad-40f2-8b10-dd349c8a0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing remaining components\n",
    "import json\n",
    "import urllib\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33fe3a-120a-4219-906a-5e72e8e70120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d1732-04e3-4d43-89e2-6c218791b12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3534c8d-fb73-481a-a4b9-2aa03b2cfa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f501cb-07a3-4d33-a2d8-cfed68314fd1",
   "metadata": {},
   "source": [
    "## Data Crawling for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06653f-eca5-49c1-ac8b-b30880951402",
   "metadata": {},
   "source": [
    "* Here we create each folder for classical instruments that used in classical music concert\n",
    "* bassoon / cello / clarinet / contrabass / flute / horn / oboe / percussion / tamtam / timpani / trombone / trumpet / tuba / viola / violin -- total 15 instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd079faf-61a3-49ee-819e-a0a8ab212415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import time\n",
    "# from urllib.parse import quote_plus\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "# from icrawler.builtin import GoogleImageCrawler, BingImageCrawler, BaiduImageCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78c751-f486-4b10-9586-89e6afbaad61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### image crawling from google with GoogleImageCrawler\n",
    "# google_crawler = GoogleImageCrawler(\n",
    "#     feeder_threads=1,\n",
    "#     parser_threads=1,\n",
    "#     downloader_threads=4,\n",
    "#     storage={'root_dir': 'E:/RESEARCH/Datasets/VC/classic/clarinet'})\n",
    "# #     storage={'root_dir': 'E:/RESEARCH/Datasets/image/CIFAR_PUB/truck'}) #set the storage root\n",
    "\n",
    "# filters = dict(\n",
    "#     type='photo',\n",
    "#     #type=photo,face,clipart,linedrawing,animated\n",
    "#     size='medium',\n",
    "#     #size=large, medium, icon, or larger than a given size e.g.\">640x480\" or exactly giving size\"=1024x768\n",
    "# #     color='orange',\n",
    "#     #coler=blackandwhite, red, oragne, yellow, green, teal, blue, purple, pink, white, gray, black, brown\n",
    "# #     license='commercial,modify',\n",
    "#     #license=noncommercial, commercial, noncommercial,modify , commercial,modify\n",
    "#     date=((2000, 1, 1), (2021, 12, 30)))\n",
    "\n",
    "# # type the keyword of the image that you want to crawl from google\n",
    "# google_crawler.crawl(keyword= 'orchestra clarinet', filters=filters, offset=0, max_num=1000,\n",
    "#                      min_size=(200,200), max_size=None, file_idx_offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0d74b-b4ec-4a2b-bd70-42eeb585eac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9697828-d6f6-4bb6-86a4-19c134ebe2b2",
   "metadata": {},
   "source": [
    "if using baidu or bing to crawl more dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbea21-be7d-4090-859e-75c5a2859049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### baidu crawler\n",
    "# baidu_crawler = BaiduImageCrawler(storage={'root_dir': 'E:/RESEARCH/Datasets/VC/classic/test'})\n",
    "# baidu_crawler.crawl(keyword='violin', offset=0, max_num=1000,\n",
    "#                     min_size=(200,200), max_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816b4d7-8724-41be-a777-fb9931db1df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### bing crawler\n",
    "# bing_crawler = BingImageCrawler(downloader_threads=4,\n",
    "#                                 storage={'root_dir': 'E:/RESEARCH/Datasets/VC/classic/test'})\n",
    "# bing_crawler.crawl(keyword='violin orchestra', filters=None, offset=0, max_num=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e9750-c2b0-4d58-818e-8746d383949f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b7a8c-0ae0-4d53-9ee0-f665220534e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8879929-cd2c-4d3b-9280-057144d7005f",
   "metadata": {},
   "source": [
    "## Training model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea4ea8-7e47-4e4d-bd35-e2b98cfa7342",
   "metadata": {},
   "source": [
    "* Selected Classical convert video: \n",
    "https://www.youtube.com/watch?v=65nvqmVhZ3g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a2026-b1d0-4fbf-a5fd-253be1d5dc72",
   "metadata": {},
   "source": [
    "* Our approach is train the model from image classification task, and apply it for the video classification. \n",
    "* So it is also possible to use pretrained models, such as, resnet, efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb96f5-b746-465e-b431-efd2510bb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the `slowFAST_r50` pretrained model - for our video classification model training \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fd817-eb45-4561-b124-ae4843daa154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ca06b-0d09-4417-a345-2e3583dde4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f2f91-05da-470c-8e58-94093a6c488e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad4ea5-1d90-4b54-870a-9cd518b35c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d9e67-1ca3-4986-93b9-9ac7c8ec4a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3468a1-81b8-4049-a8c6-1baa276cbb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7efbdd-f571-4407-97d6-e36fa4df54b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6f8d8-4e2b-46e1-8d19-dfb683cf1124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc5bd2-a1bf-41ce-aeb6-34f6a1dc66b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc65e8a-d2f0-4c10-a505-44f6e4f85c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bbad7d-ea5b-4934-b8ad-4baa8751efc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f528fd-c78e-4626-83f0-bfe70becd64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cd723-bda8-4ee5-b904-1aab6cff9fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524ca79-089b-4a98-8b63-980d75b964f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c67e0-7d3d-4249-a223-2854b2565a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf97a0-bdea-4dbd-8900-affa31e89ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8f03e3-271c-48f4-b5f8-afa2f6617347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f78ea22-8cf2-4ee7-9a2a-e1cdac006cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daae65ac-44eb-4963-88f6-719501d2c1d5",
   "metadata": {},
   "source": [
    "## Image classification approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe4d38-442f-48df-81b3-c1f1a7556763",
   "metadata": {},
   "source": [
    "* Generating classification model with image classification task.\n",
    "* Then we can adop the model for video input, to figure out the instrument on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b41d0-5085-4f99-889b-3c6c61434ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=30\n",
    "    bs=6\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    \n",
    "    num_channels=3\n",
    "    num_classes=15\n",
    "    verbose='store_true'\n",
    "    seed=710674\n",
    "\n",
    "args = Args()    \n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef1800-a496-472e-af6e-4677f31eaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting torch environment\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c756a-bb29-4e65-b6cf-418e5cab7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "data_transforms = transforms.Compose([\n",
    "#     transforms.CenterCrop(1024),\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "#     transforms.ColorJitter(contrast=(0.3, 1), saturation=(0.3, 1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c6155-e112-41d5-8ee8-f39f85f264c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading image data\n",
    "classic_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/VC/classic/train',\n",
    "                                    transform = data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9e65d-e2d0-4eea-bd44-6ffa539f31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(printer_data))\n",
    "test_size = len(printer_data)-train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c67f6-c6f9-4e65-b566-ee8facea7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(printer_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98a531-fd1f-414e-8b92-6c23c42749f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3e2d6-eab4-4c78-a166-abfe0a8325fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fc5c3-5bd8-49c3-bf62-673851e89322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_res = models.resnet18(num_classes=2, pretrained=True)\n",
    "model_eff3 = EfficientNet.from_pretrained('efficientnet-b3', num_classes=2)\n",
    "# model = model_res.to(DEVICE)\n",
    "model = model_eff3.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918611c-ba1f-4513-8808-d5aac0f6ead2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6251cd-6649-4313-ade0-44db0324f9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413c1d1-ba11-4cc5-992d-756dd20408b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1add59-6efb-4ce7-9257-1a37fa053352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=30, anneal_strategy='cos')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d8cc1-00ce-45cf-ab88-440af2c1fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during CNN model\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "\n",
    "    scheduler.step() #for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d4cbe-76e8-4e4c-842b-b42f5fa63606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during the learning process\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader)) \n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba61784-d863-4b83-8fb0-81b08a05dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking train, val loss and accuracy\n",
    "\n",
    "total = []\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))\n",
    "    \n",
    "    total.append((test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347e548-50ef-47da-9b59-3eb1fb09bf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8a4ba-da30-4819-9725-0f54301c208d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdfe52-ff09-41db-afe7-dd48372b0a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050baee-ca20-4b53-9a5e-fb60c462dc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2f97d5b-91d7-4a2b-96db-e5468e1d8563",
   "metadata": {},
   "source": [
    "## Classification model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7c875-e652-4d0f-8ac9-cd2c7009727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6030704-4052-4e93-9db1-cee6b8bbcdb9",
   "metadata": {},
   "source": [
    "We are using orchestra symphony video as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723fe7f-4a40-460e-ab25-2a963ed2ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for \n",
    "def create_dir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    except OSError:\n",
    "        print(f\"ERROR: creating directory with name {path}\")\n",
    "\n",
    "def save_frame(video_path, save_dir, gap=10):\n",
    "    name = video_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    save_path = os.path.join(save_dir, name)\n",
    "    create_dir(save_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "        if idx == 0:\n",
    "            cv2.imwrite(f\"{save_path}/{idx}.png\", frame)\n",
    "        else:\n",
    "            if idx % gap == 0:\n",
    "                cv2.imwrite(f\"{save_path}/{idx}.png\", frame)\n",
    "\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bc63d-62f8-4a6c-98d5-e7e964dcf743",
   "metadata": {},
   "source": [
    "Extracting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bde4ee-187b-43ed-9cf0-b0d066fac9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    video_paths = glob(\"E:/RESEARCH/Datasets/VC/classic/val/INPROCESS/*\")\n",
    "    save_dir = \"save\"\n",
    "\n",
    "    for path in video_paths:\n",
    "        save_frame(path, save_dir, gap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b957d-ca9e-41e3-a639-ac71fa014163",
   "metadata": {},
   "outputs": [],
   "source": [
    "## you have to end all windows before next step\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd6ee2-76a4-4b12-8b16-866bbc696c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99c76433-38ba-4388-a6b4-8eb69adc3a98",
   "metadata": {},
   "source": [
    "* Validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfbf78c-a79e-4e86-9836-1401b9ad26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = args.lr, momentum = args.momentum)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer = optimizer,\n",
    "#                                        lr_lambda = lambda epoch:0.95 ** epoch,\n",
    "#                                        last_epoch = -1,\n",
    "#                                        verbose = False)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, \n",
    "#                                                 steps_per_epoch=10, epochs=10,anneal_strategy='linear')\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=50,anneal_strategy='cos')\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3844b-b078-4f2b-b932-20a135e2ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = 'E:/RESEARCH/Datasets/VC/classic/val'\n",
    "test_dataset = datasets.ImageFolder(root=data_folder_path, transform=data_transforms)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12063b-0494-4fa6-936f-10f521a4235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 50):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_dataloader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46088fa8-a92a-47be-9deb-cfbda2fa6b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb6066-4e80-4ec6-9a14-99752459a48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9751e97-b927-4d61-84c8-8591d1dd0532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b04c7-1e42-45e5-8166-06a861c9e46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0872e79-725d-4d89-a171-390f9be315e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfe4dd-1e84-4916-9ba3-bd6df861d547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b673e-efc5-4efb-be6e-f73bf585a3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8256b-0b81-403b-8d9c-054e06ad6afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ea042-6746-4f43-93ef-5d570076c6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779180e1-b001-4d55-82e6-d6531a715f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886639e-e9f1-4a77-9adc-6771b4a95c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5a30d-df1f-4e67-b816-06e082be8df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
