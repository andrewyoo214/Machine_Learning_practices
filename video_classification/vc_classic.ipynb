{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fd5ae6-fddc-4a58-893a-5998feb39f8d",
   "metadata": {},
   "source": [
    "# Video Classification on CM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1da5cc-5ee8-4987-b5b4-0225c4c4884a",
   "metadata": {},
   "source": [
    "* Here we generate real-time classical music instruments information generator on classical music concert video input. \n",
    "* We basically adopt video classification method using pytorch, to classify what instrument is currently viewed on the screen. \n",
    "* Audience will receive the streaming video with information on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2f13d-b751-498b-979d-1c54d8af3c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f896f26-7f7e-4751-a302-21bd10505fc1",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fd2e4-6ec5-430b-86b0-3f5a458afdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required components \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import urllib\n",
    "import random\n",
    "import torchvision\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f9db6-a61a-4e36-a3c7-5ed9dde9654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c56b5c-1d80-48b1-a1d0-384dd1c944d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras.applications import ResNet50\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.models import Model\n",
    "from pickle import dump\n",
    "from keras.optimizers import SGD\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a75ce7-9dad-40f2-8b10-dd349c8a0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing remaining components\n",
    "# import json\n",
    "# import urllib\n",
    "# from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "# from torchvision.transforms import Compose, Lambda\n",
    "# from torchvision.transforms._transforms_video import (\n",
    "#     CenterCropVideo,\n",
    "#     NormalizeVideo,\n",
    "# )\n",
    "# from pytorchvideo.transforms import (\n",
    "#     ApplyTransformToKey,\n",
    "#     ShortSideScale,\n",
    "#     UniformTemporalSubsample\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b41d0-5085-4f99-889b-3c6c61434ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=30\n",
    "    bs=16\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    num_channels=3  # due to RGB channels(image)\n",
    "    num_classes=17  # total 17 instruments labeled\n",
    "    verbose='store_true'\n",
    "    seed=710674\n",
    "\n",
    "args = Args()    \n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef1800-a496-472e-af6e-4677f31eaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting torch environment\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33fe3a-120a-4219-906a-5e72e8e70120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3534c8d-fb73-481a-a4b9-2aa03b2cfa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f501cb-07a3-4d33-a2d8-cfed68314fd1",
   "metadata": {},
   "source": [
    "## Data Crawling for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06653f-eca5-49c1-ac8b-b30880951402",
   "metadata": {},
   "source": [
    "* Here we create each folder for classical instruments that used in classical music concert\n",
    "* bassdrum / bassoon / cello / clarinet / contrabass / cymbales / flute / horn / oboe / snaredrum / percussion / tamtam / timpani / trombone / trumpet / tuba / viola / violin -- total 17 instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1a9f6-a19b-442c-88d9-fb0514208317",
   "metadata": {},
   "source": [
    "### Image extraction from classical music concert video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410dcdb-37c7-44a7-bf7c-f5c3e91d9e40",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7c875-e652-4d0f-8ac9-cd2c7009727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6030704-4052-4e93-9db1-cee6b8bbcdb9",
   "metadata": {},
   "source": [
    "We are using orchestra symphony video as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723fe7f-4a40-460e-ab25-2a963ed2ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Code for \n",
    "# def create_dir(path):\n",
    "#     try:\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#     except OSError:\n",
    "#         print(f\"ERROR: creating directory with name {path}\")\n",
    "\n",
    "# def save_frame(video_path, save_dir, gap=10):\n",
    "#     name = video_path.split(\"/\")[-1].split(\".\")[0]\n",
    "#     save_path = os.path.join(save_dir, name)\n",
    "#     create_dir(save_path)\n",
    "\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     idx = 0\n",
    "\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "\n",
    "#         if ret == False:\n",
    "#             cap.release()\n",
    "#             break\n",
    "\n",
    "#         if idx == 0:\n",
    "#             cv2.imwrite(f\"{save_path}/{idx}_Saint.png\", frame)\n",
    "#         else:\n",
    "#             if idx % gap == 0:\n",
    "#                 cv2.imwrite(f\"{save_path}/{idx}_Saint.png\", frame)\n",
    "\n",
    "#         idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bc63d-62f8-4a6c-98d5-e7e964dcf743",
   "metadata": {},
   "source": [
    "Extracting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bde4ee-187b-43ed-9cf0-b0d066fac9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     video_paths = glob(\"E:/RESEARCH/Datasets/VC/classic/videos/INPROCESS/*\")\n",
    "#     save_dir = \"save\"\n",
    "\n",
    "#     for path in video_paths:\n",
    "#         save_frame(path, save_dir, gap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b957d-ca9e-41e3-a639-ac71fa014163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## you have to end all windows before next step\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593329dc-0b73-43f3-8599-407288bcb026",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d9628-3fdb-414a-b351-0291143b2aa2",
   "metadata": {},
   "source": [
    "* Changing file names in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1190a95-3653-44fc-b391-b04da2a4beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"E:/RESEARCH/Datasets/VC/classic/train/woodwinds\"\n",
    "# file_names = os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d58385-7a04-4b37-9f77-814f092c4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# for name in file_names:\n",
    "#     src = os.path.join(file_path, name)\n",
    "#     dst = str(i) + '.png'\n",
    "#     dst = os.path.join(file_path, dst)\n",
    "#     os.rename(src, dst)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab962990-e8cf-475f-88a7-7673b34962e6",
   "metadata": {},
   "source": [
    "* Image Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac06aff-2fcc-4121-aa03-7a7010c25a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = Image.open(r\"./test2.png\")\n",
    " \n",
    "# # Setting the points for cropped image\n",
    "# left = 1000\n",
    "# top = 200\n",
    "# right =3000\n",
    "# bottom = 3000\n",
    " \n",
    "# # Cropped image of above dimension\n",
    "# # (It will not change original image)\n",
    "# im1 = im.crop((left, top, right, bottom))\n",
    " \n",
    "# # Shows the image in image viewer\n",
    "# im1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b103082-e080-49e6-ace1-cbe7a5759b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4e73a54-0c58-416a-9aad-dc1c61e75418",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8879929-cd2c-4d3b-9280-057144d7005f",
   "metadata": {},
   "source": [
    "## Training model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea4ea8-7e47-4e4d-bd35-e2b98cfa7342",
   "metadata": {},
   "source": [
    "* Selected Classical convert video: \n",
    "https://www.youtube.com/watch?v=65nvqmVhZ3g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a2026-b1d0-4fbf-a5fd-253be1d5dc72",
   "metadata": {},
   "source": [
    "* Our approach is train the model from image classification task, and apply it for the video classification. \n",
    "* So it is also possible to use pretrained models, such as, resnet, efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6adf17-7cdb-43d3-b224-eaad3b1dadf0",
   "metadata": {},
   "source": [
    "### Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb96f5-b746-465e-b431-efd2510bb4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Choose the `slowFAST_r50` pretrained model - for our video classification model training \n",
    "# ## slowfast net is for video classification. \n",
    "# model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fd817-eb45-4561-b124-ae4843daa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_res = models.resnet18(num_classes=2, pretrained=True)\n",
    "model_eff3 = EfficientNet.from_pretrained('efficientnet-b3', num_classes=args.num_classes)\n",
    "model_resnet18 = models.resnet18(pretrained=True)\n",
    "model_mobnetv2 = models.mobilenet_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf97a0-bdea-4dbd-8900-affa31e89ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18.fc = nn.Linear(in_features = 512, out_features = args.num_classes)\n",
    "model_mobnetv2.classifier = nn.Linear(in_features = 1280, out_features=args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fecf7-5416-42e9-bfc7-b8bb844c5463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28af4ae2-7ad0-4872-bbc1-d2ff9822aee1",
   "metadata": {},
   "source": [
    "### Simple CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b9fde-d4a8-4598-9f96-3e3843a951bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Designing simple CNN model architecture.\n",
    "class CNN_vc(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(CNN_vc, self).__init__()\n",
    "\n",
    "        def conv_batch(input_size, output_size, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_size, output_size, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(output_size),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "\n",
    "        def conv_depth(input_size, output_size, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_size, input_size, 3, stride, 1, groups=input_size, bias=False),\n",
    "                nn.BatchNorm2d(input_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                \n",
    "                nn.Conv2d(input_size, output_size, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(output_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_batch(3, 32, 2),\n",
    "            conv_depth(32, 64, 1),\n",
    "            conv_depth(64, 128, 2),\n",
    "            conv_depth(128, 128, 1),\n",
    "            conv_depth(128, 256, 2),\n",
    "            conv_depth(256, 256, 1),\n",
    "            conv_depth(256, 512, 2),\n",
    "            conv_depth(512, 512, 1),\n",
    "            conv_depth(512, 512, 1),\n",
    "            conv_depth(512, 1024, 2),\n",
    "            conv_depth(1024, 1024, 1),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "#         self.fc1 = nn.Linear(1024, 100)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "#         x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8a1f1-85d7-4fba-908b-1aafa65b4647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fca1d-7d4f-4d01-a353-0df12e954c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163abf6-c747-4344-9183-50f31137992a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f46f732b-636b-4f3e-9d30-1fcec34e2817",
   "metadata": {},
   "source": [
    "* Finalize the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2c31d-9561-47e8-b25a-abed579a9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_eff3.to(DEVICE)\n",
    "# model = model_mobnetv2.to(DEVICE)\n",
    "# model = model_resnet18.to(DEVICE)\n",
    "model = CNN_vc(args.num_channels, num_classes = args.num_classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95355938-0d36-4af6-a6ba-02d4f16a472c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae1ef0-9efd-473f-bbb0-a7cc64801d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859de6d-70ca-4f33-a9c3-2b2c81eda599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daae65ac-44eb-4963-88f6-719501d2c1d5",
   "metadata": {},
   "source": [
    "## Image classification approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe4d38-442f-48df-81b3-c1f1a7556763",
   "metadata": {},
   "source": [
    "* Generating classification model with image classification task.\n",
    "* Then we can adop the model for video input, to figure out the instrument on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d89670-57ce-41d0-84d2-65c20699bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(train_loader):\n",
    "\n",
    "    cnt = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "\n",
    "    for images, _ in train_loader:\n",
    "\n",
    "        b, c, h, w = images.shape\n",
    "        nb_pixels = b * h * w\n",
    "        sum_ = torch.sum(images, dim=[0, 2, 3])\n",
    "        sum_of_square = torch.sum(images ** 2, dim=[0, 2, 3])\n",
    "        fst_moment = (cnt * fst_moment + sum_) / (cnt + nb_pixels)\n",
    "        snd_moment = (cnt * snd_moment + sum_of_square) / (cnt + nb_pixels)\n",
    "\n",
    "        cnt += nb_pixels\n",
    "\n",
    "    return fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa001b5-174c-477f-9178-f4b3099c159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_mean, classic_std = normalize_image(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9704e39-fadf-4ed8-8b8a-d39e685ddb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classic_mean, classic_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2741bcb-afec-4ef6-8c04-178c8a5f4a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c018b93-ead3-43b5-b78c-aac24021c071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c756a-bb29-4e65-b6cf-418e5cab7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "data_transforms = transforms.Compose([\n",
    "#     transforms.CenterCrop(1024),\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(contrast=(0.3, 1), saturation=(0.3, 1)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c6155-e112-41d5-8ee8-f39f85f264c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading image data\n",
    "classic_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/VC/classic/train', transform = data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9e65d-e2d0-4eea-bd44-6ffa539f31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(classic_data))\n",
    "test_size = len(classic_data)-train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c67f6-c6f9-4e65-b566-ee8facea7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(classic_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98a531-fd1f-414e-8b92-6c23c42749f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3e2d6-eab4-4c78-a166-abfe0a8325fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918611c-ba1f-4513-8808-d5aac0f6ead2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413c1d1-ba11-4cc5-992d-756dd20408b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1add59-6efb-4ce7-9257-1a37fa053352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=30, anneal_strategy='cos')\n",
    "criterion = nn.CrossEntropyLoss() ## setup the loss function\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d8cc1-00ce-45cf-ab88-440af2c1fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during CNN model\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "\n",
    "    scheduler.step() #for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d4cbe-76e8-4e4c-842b-b42f5fa63606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during the learning process\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    validation =[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader)) \n",
    "    validation_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    validation.append(validation_accuracy)\n",
    "    \n",
    "    return test_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba61784-d863-4b83-8fb0-81b08a05dbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking train, val loss and accuracy\n",
    "\n",
    "test_los_total = []\n",
    "vali_acc_total = []\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, validation_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tValidation Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, validation_accuracy))\n",
    "    \n",
    "    test_los_total.append(test_loss)\n",
    "    vali_acc_total.append(validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347e548-50ef-47da-9b59-3eb1fb09bf9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_los_total)\n",
    "print(val_acc_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53344f97-ab5f-47be-9b9c-c7ed44d61816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2405b4-1ad4-40d7-8d2b-abe875606735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "372e285e-26cc-457f-98af-5e1f06e132a1",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae786f58-3d41-4662-996f-ddcb8a17a4cb",
   "metadata": {},
   "source": [
    "## Model Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b6e4a-10cc-423e-94ef-61b495df80b2",
   "metadata": {},
   "source": [
    "Performance Measure Explanations\n",
    "\n",
    "* Sensitivity = TP/(TP+FN) = (Number of true positive assessment) / (Number of all posi tive assessment)\n",
    "* Specificity = TN/(TN + FP) = (Number of true negative assessment)/(Number of all negative assessment)\n",
    "* Accuracy = (TN + TP)/(TN+TP+FN+FP) = (Number of correct assessments)/Number of all assessments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e028f4-9af4-4bd7-a7d7-b0920d23d70d",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8a4ba-da30-4819-9725-0f54301c208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = args.num_classes\n",
    "confusion_matrix = np.zeros((nb_classes, nb_classes))\n",
    "classes = {\n",
    "    \"0\": \"Bassoon\",     \"1\": \"Brass\",        \"2\": \"Cello\",    \"3\": \"Clarinet\",\n",
    "    \"4\": \"Double Bass\", \"5\": \"English Horn\", \"6\": \"Flute\",    \"7\": \"Horn\",\n",
    "    \"8\": \"Oboe\",        \"9\": \"Percussion\",   \"10\": \"Piccolo\", \"11\": \"Strings\",\n",
    "    \"12\": \"Timpani\",    \"13\": \"Trombone\",    \"14\": \"Trumpet\", \"15\": \"Tuba\",\n",
    "    \"16\": \"Woodwinds\"\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (image, label) in enumerate(test_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        outputs = model(image)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(label.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "# print(confusion_matrix)\n",
    "\n",
    "class_names = list(classes.values())\n",
    "df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=8)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=8)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "# plt.savefig('Classical Music_VC_output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87577c84-7e12-45ea-b6a7-0e2b09e23985",
   "metadata": {},
   "source": [
    "### Accuracy and Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdfe52-ff09-41db-afe7-dd48372b0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix\n",
    "total = sum(sum(cm))\n",
    "print(\"Total number of test data is: \", total)\n",
    "\n",
    "## Accuracy\n",
    "acc = (cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+cm[8,8]+\n",
    "       cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+cm[16,16]) / total\n",
    "\n",
    "print(\"Overall classification accuracy is :\", round(acc, 4))\n",
    "\n",
    "\n",
    "## Sensitivity for each instrument classes\n",
    "sen_bassoon = cm[0,0] / (cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]+cm[0,4]+cm[0,5]+cm[0,6]+\n",
    "                         cm[0,7]+cm[0,8]+cm[0,9]+cm[0,10]+cm[0,11]+cm[0,12]+cm[0,13]+\n",
    "                         cm[0,14]+cm[0,15]+cm[0,16])\n",
    "\n",
    "sen_brass = cm[1,1] / (cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]+cm[1,4]+cm[1,5]+cm[1,6]+\n",
    "                         cm[1,7]+cm[1,8]+cm[1,9]+cm[1,10]+cm[1,11]+cm[1,12]+cm[1,13]+\n",
    "                         cm[1,14]+cm[1,15]+cm[1,16])\n",
    "\n",
    "sen_cello = cm[2,2] / (cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]+cm[2,4]+cm[2,5]+cm[2,6]+\n",
    "                         cm[2,7]+cm[2,8]+cm[2,9]+cm[2,10]+cm[2,11]+cm[2,12]+cm[2,13]+\n",
    "                         cm[2,14]+cm[2,15]+cm[2,16])\n",
    "\n",
    "sen_clarinet = cm[3,3] / (cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]+cm[3,4]+cm[3,5]+cm[3,6]+\n",
    "                         cm[3,7]+cm[3,8]+cm[3,9]+cm[3,10]+cm[3,11]+cm[3,12]+cm[3,13]+\n",
    "                         cm[3,14]+cm[3,15]+cm[3,16])\n",
    "\n",
    "sen_doublebass = cm[4,4] / (cm[4,0]+cm[4,1]+cm[4,2]+cm[4,3]+cm[4,4]+cm[4,5]+cm[4,6]+\n",
    "                         cm[4,7]+cm[4,8]+cm[4,9]+cm[4,10]+cm[4,11]+cm[4,12]+cm[4,13]+\n",
    "                         cm[4,14]+cm[4,15]+cm[4,16])\n",
    "\n",
    "sen_englishhorn = cm[5,5] / (cm[5,0]+cm[5,1]+cm[5,2]+cm[5,3]+cm[5,4]+cm[5,5]+cm[5,6]+\n",
    "                         cm[5,7]+cm[5,8]+cm[5,9]+cm[5,10]+cm[5,11]+cm[5,12]+cm[5,13]+\n",
    "                         cm[5,14]+cm[5,15]+cm[5,16])\n",
    "\n",
    "sen_flute = cm[6,6] / (cm[6,0]+cm[6,1]+cm[6,2]+cm[6,3]+cm[6,4]+cm[6,5]+cm[6,6]+\n",
    "                         cm[6,7]+cm[6,8]+cm[6,9]+cm[6,10]+cm[6,11]+cm[6,12]+cm[6,13]+\n",
    "                         cm[6,14]+cm[6,15]+cm[6,16])\n",
    "\n",
    "sen_horn = cm[7,7] / (cm[7,0]+cm[7,1]+cm[7,2]+cm[7,3]+cm[7,4]+cm[7,5]+cm[7,6]+\n",
    "                         cm[7,7]+cm[7,8]+cm[7,9]+cm[7,10]+cm[7,11]+cm[7,12]+cm[7,13]+\n",
    "                         cm[7,14]+cm[7,15]+cm[7,16])\n",
    "\n",
    "sen_oboe = cm[8,8] / (cm[8,0]+cm[8,1]+cm[8,2]+cm[8,3]+cm[8,4]+cm[8,5]+cm[8,6]+\n",
    "                         cm[8,7]+cm[8,8]+cm[8,9]+cm[8,10]+cm[8,11]+cm[8,12]+cm[8,13]+\n",
    "                         cm[8,14]+cm[8,15]+cm[8,16])\n",
    "\n",
    "sen_percussion = cm[9,9] / (cm[9,0]+cm[9,1]+cm[9,2]+cm[9,3]+cm[9,4]+cm[9,5]+cm[9,6]+\n",
    "                         cm[9,7]+cm[9,8]+cm[9,9]+cm[9,10]+cm[9,11]+cm[9,12]+cm[9,13]+\n",
    "                         cm[9,14]+cm[9,15]+cm[9,16])\n",
    "\n",
    "sen_piccolo=cm[10,10] / (cm[10,0]+cm[10,1]+cm[10,2]+cm[10,3]+cm[10,4]+cm[10,5]+cm[10,6]+\n",
    "                         cm[10,7]+cm[10,8]+cm[10,9]+cm[10,10]+cm[10,11]+cm[10,12]+cm[10,13]+\n",
    "                         cm[10,14]+cm[10,15]+cm[10,16])\n",
    "\n",
    "sen_strings=cm[11,11] / (cm[11,0]+cm[11,1]+cm[11,2]+cm[11,3]+cm[11,4]+cm[11,5]+cm[11,6]+\n",
    "                         cm[11,7]+cm[11,8]+cm[11,9]+cm[11,10]+cm[11,11]+cm[11,12]+cm[11,13]+\n",
    "                         cm[11,14]+cm[11,15]+cm[11,16])\n",
    "\n",
    "sen_timpani=cm[12,12] / (cm[12,0]+cm[12,1]+cm[12,2]+cm[12,3]+cm[12,4]+cm[12,5]+cm[12,6]+\n",
    "                         cm[12,7]+cm[12,8]+cm[12,9]+cm[12,10]+cm[12,11]+cm[12,12]+cm[12,13]+\n",
    "                         cm[12,14]+cm[12,15]+cm[12,16])\n",
    "\n",
    "sen_trombone=cm[13,13] / (cm[13,0]+cm[13,1]+cm[13,2]+cm[13,3]+cm[13,4]+cm[13,5]+cm[13,6]+\n",
    "                         cm[13,7]+cm[13,8]+cm[13,9]+cm[13,10]+cm[13,11]+cm[13,12]+cm[13,13]+\n",
    "                         cm[13,14]+cm[13,15]+cm[13,16])\n",
    "\n",
    "sen_trumpet=cm[14,14] / (cm[14,0]+cm[14,1]+cm[14,2]+cm[14,3]+cm[14,4]+cm[14,5]+cm[14,6]+\n",
    "                         cm[14,7]+cm[14,8]+cm[14,9]+cm[14,10]+cm[14,11]+cm[14,12]+cm[14,13]+\n",
    "                         cm[14,14]+cm[14,15]+cm[14,16])\n",
    "\n",
    "sen_tuba=cm[15,15] / (cm[15,0]+cm[15,1]+cm[15,2]+cm[15,3]+cm[15,4]+cm[15,5]+cm[15,6]+\n",
    "                         cm[15,7]+cm[15,8]+cm[15,9]+cm[15,10]+cm[15,11]+cm[15,12]+cm[15,13]+\n",
    "                         cm[15,14]+cm[15,15]+cm[15,16])\n",
    "\n",
    "sen_woodwinds=cm[16,16] / (cm[16,0]+cm[16,1]+cm[16,2]+cm[16,3]+cm[16,4]+cm[16,5]+cm[16,6]+\n",
    "                         cm[16,7]+cm[16,8]+cm[16,9]+cm[16,10]+cm[16,11]+cm[16,12]+cm[16,13]+\n",
    "                         cm[16,14]+cm[16,15]+cm[16,16])\n",
    "\n",
    "\n",
    "print(\"sensitivity of Bassoon class is :\", round(sen_bassoon, 4))\n",
    "print(\"sensitivity of Brass class is :\", round(sen_brass,4))\n",
    "print(\"sensitivity of Cello class is :\", round(sen_cello,4))\n",
    "print(\"sensitivity of Clarinet class is :\", round(sen_clarinet, 4))\n",
    "print(\"sensitivity of Double Bass class is :\", round(sen_doublebass,4))\n",
    "print(\"sensitivity of English Horn class is :\", round(sen_englishhorn,4))\n",
    "print(\"sensitivity of Flute class is :\", round(sen_flute, 4))\n",
    "print(\"sensitivity of Horn class is :\", round(sen_horn,4))\n",
    "print(\"sensitivity of Oboe class is :\", round(sen_oboe,4))\n",
    "print(\"sensitivity of Percussion class is :\", round(sen_percussion, 4))\n",
    "print(\"sensitivity of Piccolo class is :\", round(sen_piccolo,4))\n",
    "print(\"sensitivity of Strings class is :\", round(sen_strings,4))\n",
    "print(\"sensitivity of Timpani class is :\", round(sen_timpani, 4))\n",
    "print(\"sensitivity of Trombone class is :\", round(sen_trombone,4))\n",
    "print(\"sensitivity of Trumpet class is :\", round(sen_trumpet,4))\n",
    "print(\"sensitivity of Tuba class is :\", round(sen_tuba, 4))\n",
    "print(\"sensitivity of Woodwinds class is :\", round(sen_woodwinds,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710ac6f-db05-4eb0-bcce-219353533090",
   "metadata": {},
   "source": [
    "### Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050baee-ca20-4b53-9a5e-fb60c462dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specificity for each instrument classes: TN/(TN + FP)\n",
    "spe_bas=(cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+cm[8,8]+\n",
    "         cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[1,0]+cm[2,0]+cm[3,0]+cm[4,0]+cm[5,0]+cm[6,0]+cm[7,0]+\n",
    "                       cm[8,0]+cm[9,0]+cm[10,0]+cm[11,0]+cm[12,0]+cm[13,0]+cm[14,0]+\n",
    "                       cm[15,0]+cm[16,0] + \n",
    "                       cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "                       cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_bra=(cm[0,0]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+cm[8,8]+\n",
    "         cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,1]+cm[2,1]+cm[3,1]+cm[4,1]+cm[5,1]+cm[6,1]+cm[7,1]+\n",
    "                       cm[8,1]+cm[9,1]+cm[10,1]+cm[11,1]+cm[12,1]+cm[13,1]+cm[14,1]+\n",
    "                       cm[15,1]+cm[16,1] + \n",
    "                       cm[0,0]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "                       cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_cel=(cm[0,0]+cm[1,1]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+cm[8,8]+\n",
    "         cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,2]+cm[1,2]+cm[3,2]+cm[4,2]+cm[5,2]+cm[6,2]+cm[7,2]+\n",
    "                       cm[8,2]+cm[9,2]+cm[10,2]+cm[11,2]+cm[12,2]+cm[13,2]+cm[14,2]+\n",
    "                       cm[15,2]+cm[16,2] + \n",
    "                       cm[0,0]+cm[1,1]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "                       cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_cla=(cm[0,0]+cm[1,1]+cm[2,2]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+cm[8,8]+\n",
    "         cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,3]+cm[1,3]+cm[2,3]+cm[4,3]+cm[5,3]+cm[6,3]+cm[7,3]+\n",
    "                       cm[8,3]+cm[9,3]+cm[10,3]+cm[11,3]+cm[12,3]+cm[13,3]+cm[14,3]+\n",
    "                       cm[15,3]+cm[16,3] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "                       cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_dou=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[5,5]+cm[6,6]+cm[7,7]+cm[8,8]+\n",
    "         cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,4]+cm[1,4]+cm[2,4]+cm[3,4]+cm[5,4]+cm[6,4]+cm[7,4]+\n",
    "                       cm[8,4]+cm[9,4]+cm[10,4]+cm[11,4]+cm[12,4]+cm[13,4]+cm[14,4]+\n",
    "                       cm[15,4]+cm[16,4] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "                       cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_eng=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[6,6]+cm[7,7]+cm[8,8]+\n",
    "         cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,5]+cm[1,5]+cm[2,5]+cm[3,5]+cm[4,5]+cm[6,5]+cm[7,5]+\n",
    "                       cm[8,5]+cm[9,5]+cm[10,5]+cm[11,5]+cm[12,5]+cm[13,5]+cm[14,5]+\n",
    "                       cm[15,5]+cm[16,5] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[6,6]+cm[7,7]+\n",
    "                       cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_flu=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[7,7]+cm[8,8]+\n",
    "         cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,6]+cm[1,6]+cm[2,6]+cm[3,6]+cm[4,6]+cm[5,6]+cm[7,6]+\n",
    "                       cm[8,6]+cm[9,6]+cm[10,6]+cm[11,6]+cm[12,6]+cm[13,6]+cm[14,6]+\n",
    "                       cm[15,6]+cm[16,6] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[7,7]+\n",
    "                       cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_hor=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[8,8]+\n",
    "         cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,7]+cm[1,7]+cm[2,7]+cm[3,7]+cm[4,7]+cm[5,7]+cm[6,7]+\n",
    "                       cm[8,7]+cm[9,7]+cm[10,7]+cm[11,7]+cm[12,7]+cm[13,7]+cm[14,7]+\n",
    "                       cm[15,7]+cm[16,7] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_obo=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "         cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,8]+cm[1,8]+cm[2,8]+cm[3,8]+cm[4,8]+cm[5,8]+cm[6,8]+\n",
    "                       cm[7,8]+cm[9,8]+cm[10,8]+cm[11,8]+cm[12,8]+cm[13,8]+cm[14,8]+\n",
    "                       cm[15,8]+cm[16,8] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[7,7]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_per=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "         cm[8,8]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,9]+cm[1,9]+cm[2,9]+cm[3,9]+cm[4,9]+cm[5,9]+cm[6,9]+\n",
    "                       cm[7,9]+cm[8,9]+cm[10,9]+cm[11,9]+cm[12,9]+cm[13,9]+cm[14,9]+\n",
    "                       cm[15,9]+cm[16,9] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[7,7]+cm[8,8]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_pic=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "         cm[8,8]+cm[9,9]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,10]+cm[1,10]+cm[2,10]+cm[3,10]+cm[4,10]+cm[5,10]+cm[6,10]+\n",
    "                       cm[7,10]+cm[8,10]+cm[9,10]+cm[11,10]+cm[12,10]+cm[13,10]+cm[14,10]+\n",
    "                       cm[15,10]+cm[16,10] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[7,7]+cm[8,8]+cm[9,9]+cm[11,11]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_str=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "         cm[8,8]+cm[9,9]+cm[10,10]+cm[12,12]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,11]+cm[1,11]+cm[2,11]+cm[3,11]+cm[4,11]+cm[5,11]+cm[6,11]+\n",
    "                       cm[7,11]+cm[8,11]+cm[9,11]+cm[10,11]+cm[12,11]+cm[13,11]+cm[14,11]+\n",
    "                       cm[15,11]+cm[16,11] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[7,7]+cm[8,8]+cm[9,9]+cm[10,10]+cm[12,12]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_tim=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "         cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[13,13]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,12]+cm[1,12]+cm[2,12]+cm[3,12]+cm[4,12]+cm[5,12]+cm[6,12]+\n",
    "                       cm[7,12]+cm[8,12]+cm[9,12]+cm[10,12]+cm[11,12]+cm[13,12]+cm[14,12]+\n",
    "                       cm[15,12]+cm[16,12] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[7,7]+cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[13,13]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_tro=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "         cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[14,14]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,13]+cm[1,13]+cm[2,13]+cm[3,13]+cm[4,13]+cm[5,13]+cm[6,13]+\n",
    "                       cm[7,13]+cm[8,13]+cm[9,13]+cm[10,13]+cm[11,13]+cm[12,13]+cm[14,13]+\n",
    "                       cm[15,13]+cm[16,13] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[7,7]+cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+\n",
    "                       cm[14,14]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_tru=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "         cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[15,15]+\n",
    "         cm[16,16]) / (cm[0,14]+cm[1,14]+cm[2,14]+cm[3,14]+cm[4,14]+cm[5,14]+cm[6,14]+\n",
    "                       cm[7,14]+cm[8,14]+cm[9,14]+cm[10,14]+cm[11,14]+cm[12,14]+cm[13,14]+\n",
    "                       cm[15,14]+cm[16,14] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[7,7]+cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+\n",
    "                       cm[13,13]+cm[15,15]+cm[16,16])\n",
    "\n",
    "spe_tub=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "         cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+\n",
    "         cm[16,16]) / (cm[0,15]+cm[1,15]+cm[2,15]+cm[3,15]+cm[4,15]+cm[5,15]+cm[6,15]+\n",
    "                       cm[7,15]+cm[8,15]+cm[9,15]+cm[10,15]+cm[11,15]+cm[12,15]+cm[13,15]+\n",
    "                       cm[14,15]+cm[16,15] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[7,7]+cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+\n",
    "                       cm[13,13]+cm[14,14]+cm[16,16])\n",
    "\n",
    "spe_woo=(cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+cm[7,7]+\n",
    "         cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+cm[13,13]+cm[14,14]+\n",
    "         cm[15,15]) / (cm[0,16]+cm[1,16]+cm[2,16]+cm[3,16]+cm[4,16]+cm[5,16]+cm[6,16]+\n",
    "                       cm[7,16]+cm[8,16]+cm[9,16]+cm[10,16]+cm[11,16]+cm[12,16]+cm[13,16]+\n",
    "                       cm[14,16]+cm[15,16] + \n",
    "                       cm[0,0]+cm[1,1]+cm[2,2]+cm[3,3]+cm[4,4]+cm[5,5]+cm[6,6]+\n",
    "                       cm[7,7]+cm[8,8]+cm[9,9]+cm[10,10]+cm[11,11]+cm[12,12]+\n",
    "                       cm[13,13]+cm[14,14]+cm[15,15])\n",
    "\n",
    "\n",
    "print(\"specificity of Basson class is :\", round(spe_bas,4))\n",
    "print(\"specificity of Brass class is :\", round(spe_bra,4))\n",
    "print(\"specificity of Cello class is :\", round(spe_cel,4))\n",
    "print(\"specificity of Clarinet class is :\", round(spe_cla,4))\n",
    "print(\"specificity of Double Bass class is :\", round(spe_dou,4))\n",
    "print(\"specificity of English Horn class is :\", round(spe_eng,4))\n",
    "print(\"specificity of Flute class is :\", round(spe_flu,4))\n",
    "print(\"specificity of Horn class is :\", round(spe_hor,4))\n",
    "print(\"specificity of Oboe class is :\", round(spe_obo,4))\n",
    "print(\"specificity of Percussion class is :\", round(spe_per,4))\n",
    "print(\"specificity of Piccolo class is :\", round(spe_pic,4))\n",
    "print(\"specificity of Strings class is :\", round(spe_str,4))\n",
    "print(\"specificity of Timpani class is :\", round(spe_tim,4))\n",
    "print(\"specificity of Trombone class is :\", round(spe_tro,4))\n",
    "print(\"specificity of Trumpet class is :\", round(spe_tru,4))\n",
    "print(\"specificity of Tuba class is :\", round(spe_tub,4))\n",
    "print(\"specificity of Woodwinds class is :\", round(spe_woo,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63222323-68ff-4ad1-be93-b9ae00caf461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53b94ee9-4c4d-44b5-bec8-6230550846b8",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4249f15-dcdb-4e47-86eb-f6beaf866127",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lim = 70\n",
    "\n",
    "plt.rc('font', family='Times New Roman', serif='Times')\n",
    "#plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('axes', labelsize=20)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(left=.15, bottom=-1.16, right=1.99, top=.97)\n",
    "\n",
    "plt.plot(range(args.epochs -1), vali_acc_total, '-r')\n",
    "# plt.plot(range(29), test_los_total, '-b')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_xlabel('Number of Epochs')\n",
    "ax.legend(['Train Accuracy', 'Test Accuracy'],fontsize=15)\n",
    "\n",
    "# plt.savefig('test_acc.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e26278-e6e2-4e64-b7c7-6102c7bc6620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80685a7e-343c-4e01-9682-832aec7962ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd89ed3-e2f3-495a-944b-b73b235317d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b9d75-fd35-4ddb-be75-6332b959a257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffb53eb0-69c9-489f-bfbd-14ad0895f690",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f97d5b-91d7-4a2b-96db-e5468e1d8563",
   "metadata": {},
   "source": [
    "## Classification model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8e6d2-2324-4d2a-8717-6e600a518e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving pytorch model\n",
    "\n",
    "torch.save(model.state_dict(), 'C:/Users/user/Jupyter/ML_practices/vc_classic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e75923-fe6f-49ea-b226-05982b71cc49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_test = torch.load('C:/Users/user/Jupyter/ML_practices/vc_classic.pt')\n",
    "# print(model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c76433-38ba-4388-a6b4-8eb69adc3a98",
   "metadata": {},
   "source": [
    "* Model performance test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfbf78c-a79e-4e86-9836-1401b9ad26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=30,anneal_strategy='cos')\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3844b-b078-4f2b-b932-20a135e2ae14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder_path = 'E:/RESEARCH/Datasets/VC/classic/val'\n",
    "# data_folder_path = 'E:/RESEARCH/Datasets/VC/sports/data/val'\n",
    "test_dataset = datasets.ImageFolder(root=data_folder_path, transform=data_transforms)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12063b-0494-4fa6-936f-10f521a4235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model testing\n",
    "test_los_total = []\n",
    "test_acc_total = []\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model_test, test_dataloader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))\n",
    "    \n",
    "    test_los_total.append(test_loss)\n",
    "    test_acc_total.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46088fa8-a92a-47be-9deb-cfbda2fa6b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb6066-4e80-4ec6-9a14-99752459a48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9751e97-b927-4d61-84c8-8591d1dd0532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca8787-7f08-4555-af7e-88cdaf19e423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd47279-0ed2-4178-a92d-835d480aafaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386bad0-dcc4-4ee9-a90b-bbaaa9faabae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e6e4e-fa0a-4e7f-809a-ff384d90cced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f060c6d8-a442-4035-967d-554d72b948bd",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be39ddbf-920c-4acc-8190-0526fc968af2",
   "metadata": {},
   "source": [
    "## Real-time classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0872e79-725d-4d89-a171-390f9be315e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath= r\"E:\\RESEARCH\\Datasets\\VC\\sports\\data\"\n",
    "outputmodel = r\"E:\\RESEARCH\\Datasets\\VC\\sports\\classificationModel\"\n",
    "outputlabelbinarizer = \"E:\\RESEARCH\\Datasets\\VC\\sports\\classificationBinarizer\"\n",
    "epochs=args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfe4dd-1e84-4916-9ba3-bd6df861d547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b673e-efc5-4efb-be6e-f73bf585a3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8256b-0b81-403b-8d9c-054e06ad6afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ea042-6746-4f43-93ef-5d570076c6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779180e1-b001-4d55-82e6-d6531a715f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886639e-e9f1-4a77-9adc-6771b4a95c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5a30d-df1f-4e67-b816-06e082be8df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
