{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fd5ae6-fddc-4a58-893a-5998feb39f8d",
   "metadata": {},
   "source": [
    "# Video Classification on CM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1da5cc-5ee8-4987-b5b4-0225c4c4884a",
   "metadata": {},
   "source": [
    "* Here we generate real-time classical music instruments information generator on classical music concert video input. \n",
    "* We basically adopt video classification method using pytorch, to classify what instrument is currently viewed on the screen. \n",
    "* Audience will receive the streaming video with information on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2f13d-b751-498b-979d-1c54d8af3c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f896f26-7f7e-4751-a302-21bd10505fc1",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fd2e4-6ec5-430b-86b0-3f5a458afdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required components \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import urllib\n",
    "import random\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c56b5c-1d80-48b1-a1d0-384dd1c944d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a75ce7-9dad-40f2-8b10-dd349c8a0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing remaining components\n",
    "# import json\n",
    "# import urllib\n",
    "# from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "# from torchvision.transforms import Compose, Lambda\n",
    "# from torchvision.transforms._transforms_video import (\n",
    "#     CenterCropVideo,\n",
    "#     NormalizeVideo,\n",
    "# )\n",
    "# from pytorchvideo.transforms import (\n",
    "#     ApplyTransformToKey,\n",
    "#     ShortSideScale,\n",
    "#     UniformTemporalSubsample\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b41d0-5085-4f99-889b-3c6c61434ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=20\n",
    "    bs=6\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    num_channels=3  # due to RGB channels(image)\n",
    "    num_classes=17  # total 17 instruments labeled\n",
    "    verbose='store_true'\n",
    "    seed=710674\n",
    "\n",
    "args = Args()    \n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef1800-a496-472e-af6e-4677f31eaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting torch environment\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33fe3a-120a-4219-906a-5e72e8e70120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3534c8d-fb73-481a-a4b9-2aa03b2cfa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f501cb-07a3-4d33-a2d8-cfed68314fd1",
   "metadata": {},
   "source": [
    "## Data Crawling for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06653f-eca5-49c1-ac8b-b30880951402",
   "metadata": {},
   "source": [
    "* Here we create each folder for classical instruments that used in classical music concert\n",
    "* bassdrum / bassoon / cello / clarinet / contrabass / cymbales / flute / horn / oboe / snaredrum / percussion / tamtam / timpani / trombone / trumpet / tuba / viola / violin -- total 17 instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1a9f6-a19b-442c-88d9-fb0514208317",
   "metadata": {},
   "source": [
    "### Image extraction from classical music concert video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410dcdb-37c7-44a7-bf7c-f5c3e91d9e40",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7c875-e652-4d0f-8ac9-cd2c7009727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6030704-4052-4e93-9db1-cee6b8bbcdb9",
   "metadata": {},
   "source": [
    "We are using orchestra symphony video as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723fe7f-4a40-460e-ab25-2a963ed2ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for \n",
    "def create_dir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    except OSError:\n",
    "        print(f\"ERROR: creating directory with name {path}\")\n",
    "\n",
    "def save_frame(video_path, save_dir, gap=10):\n",
    "    name = video_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    save_path = os.path.join(save_dir, name)\n",
    "    create_dir(save_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "        if idx == 0:\n",
    "            cv2.imwrite(f\"{save_path}/{idx}.png\", frame)\n",
    "        else:\n",
    "            if idx % gap == 0:\n",
    "                cv2.imwrite(f\"{save_path}/{idx}.png\", frame)\n",
    "\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bc63d-62f8-4a6c-98d5-e7e964dcf743",
   "metadata": {},
   "source": [
    "Extracting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bde4ee-187b-43ed-9cf0-b0d066fac9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    video_paths = glob(\"E:/RESEARCH/Datasets/VC/classic/val/INPROCESS/*\")\n",
    "    save_dir = \"save\"\n",
    "\n",
    "    for path in video_paths:\n",
    "        save_frame(path, save_dir, gap=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b957d-ca9e-41e3-a639-ac71fa014163",
   "metadata": {},
   "outputs": [],
   "source": [
    "## you have to end all windows before next step\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593329dc-0b73-43f3-8599-407288bcb026",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b7a8c-0ae0-4d53-9ee0-f665220534e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1190a95-3653-44fc-b391-b04da2a4beb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d58385-7a04-4b37-9f77-814f092c4b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eebb739-a370-4d74-a087-9d68e8408761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac06aff-2fcc-4121-aa03-7a7010c25a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8879929-cd2c-4d3b-9280-057144d7005f",
   "metadata": {},
   "source": [
    "## Training model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea4ea8-7e47-4e4d-bd35-e2b98cfa7342",
   "metadata": {},
   "source": [
    "* Selected Classical convert video: \n",
    "https://www.youtube.com/watch?v=65nvqmVhZ3g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a2026-b1d0-4fbf-a5fd-253be1d5dc72",
   "metadata": {},
   "source": [
    "* Our approach is train the model from image classification task, and apply it for the video classification. \n",
    "* So it is also possible to use pretrained models, such as, resnet, efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb96f5-b746-465e-b431-efd2510bb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the `slowFAST_r50` pretrained model - for our video classification model training \n",
    "## slowfast net is for video classification. \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fd817-eb45-4561-b124-ae4843daa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_res = models.resnet18(num_classes=2, pretrained=True)\n",
    "model_eff3 = EfficientNet.from_pretrained('efficientnet-b3', num_classes=args.num_classes)\n",
    "# model = model_res.to(DEVICE)\n",
    "model = model_eff3.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ca06b-0d09-4417-a345-2e3583dde4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf97a0-bdea-4dbd-8900-affa31e89ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8f03e3-271c-48f4-b5f8-afa2f6617347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f78ea22-8cf2-4ee7-9a2a-e1cdac006cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daae65ac-44eb-4963-88f6-719501d2c1d5",
   "metadata": {},
   "source": [
    "## Image classification approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe4d38-442f-48df-81b3-c1f1a7556763",
   "metadata": {},
   "source": [
    "* Generating classification model with image classification task.\n",
    "* Then we can adop the model for video input, to figure out the instrument on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c756a-bb29-4e65-b6cf-418e5cab7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "data_transforms = transforms.Compose([\n",
    "#     transforms.CenterCrop(1024),\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(contrast=(0.3, 1), saturation=(0.3, 1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456,0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c6155-e112-41d5-8ee8-f39f85f264c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading image data\n",
    "# classic_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/VC/classic/train', transform = data_transforms)\n",
    "sports_data = datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/VC/sports/data/train', transform = data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9e65d-e2d0-4eea-bd44-6ffa539f31af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(sports_data))\n",
    "test_size = len(sports_data)-train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c67f6-c6f9-4e65-b566-ee8facea7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(sports_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98a531-fd1f-414e-8b92-6c23c42749f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3e2d6-eab4-4c78-a166-abfe0a8325fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918611c-ba1f-4513-8808-d5aac0f6ead2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6251cd-6649-4313-ade0-44db0324f9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413c1d1-ba11-4cc5-992d-756dd20408b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1add59-6efb-4ce7-9257-1a37fa053352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=20, anneal_strategy='cos')\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d8cc1-00ce-45cf-ab88-440af2c1fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during CNN model\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "\n",
    "    scheduler.step() #for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d4cbe-76e8-4e4c-842b-b42f5fa63606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during the learning process\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    validation =[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader)) \n",
    "    validation_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    validation.append(validation_accuracy)\n",
    "    \n",
    "    return test_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba61784-d863-4b83-8fb0-81b08a05dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking train, val loss and accuracy\n",
    "\n",
    "total = []\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, validation_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tValidation Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, validation_accuracy))\n",
    "    \n",
    "    total.append((test_loss, validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347e548-50ef-47da-9b59-3eb1fb09bf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8a4ba-da30-4819-9725-0f54301c208d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdfe52-ff09-41db-afe7-dd48372b0a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050baee-ca20-4b53-9a5e-fb60c462dc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2f97d5b-91d7-4a2b-96db-e5468e1d8563",
   "metadata": {},
   "source": [
    "## Classification model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8e6d2-2324-4d2a-8717-6e600a518e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e75923-fe6f-49ea-b226-05982b71cc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99c76433-38ba-4388-a6b4-8eb69adc3a98",
   "metadata": {},
   "source": [
    "* Model performance test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfbf78c-a79e-4e86-9836-1401b9ad26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = args.lr, momentum = args.momentum)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer = optimizer,\n",
    "#                                        lr_lambda = lambda epoch:0.95 ** epoch,\n",
    "#                                        last_epoch = -1,\n",
    "#                                        verbose = False)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, \n",
    "#                                                 steps_per_epoch=10, epochs=10,anneal_strategy='linear')\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=35,anneal_strategy='cos')\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3844b-b078-4f2b-b932-20a135e2ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder_path = 'E:/RESEARCH/Datasets/VC/classic/val'\n",
    "data_folder_path = 'E:/RESEARCH/Datasets/VC/sports/data/val'\n",
    "test_dataset = datasets.ImageFolder(root=data_folder_path, transform=data_transforms)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12063b-0494-4fa6-936f-10f521a4235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model testing\n",
    "total_evaluation = []\n",
    "\n",
    "for epoch in range(1, 35):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_dataloader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))\n",
    "    \n",
    "    total_evaluation.append((test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46088fa8-a92a-47be-9deb-cfbda2fa6b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb6066-4e80-4ec6-9a14-99752459a48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9751e97-b927-4d61-84c8-8591d1dd0532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b04c7-1e42-45e5-8166-06a861c9e46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0872e79-725d-4d89-a171-390f9be315e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfe4dd-1e84-4916-9ba3-bd6df861d547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b673e-efc5-4efb-be6e-f73bf585a3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8256b-0b81-403b-8d9c-054e06ad6afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ea042-6746-4f43-93ef-5d570076c6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779180e1-b001-4d55-82e6-d6531a715f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886639e-e9f1-4a77-9adc-6771b4a95c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5a30d-df1f-4e67-b816-06e082be8df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
