{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecd52dc-3d90-4c92-b374-6c8b2f09d9af",
   "metadata": {},
   "source": [
    "# Video Classification with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db17dcd-7ea2-493b-aec9-d8472ebb1abd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. PyTorch 3D RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f20188-ce06-43ac-b19b-b53a8c56d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required components \n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9defd4af-ebbe-4344-b443-fa29b971967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing remaining components\n",
    "import json\n",
    "import urllib\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15474af7-9d3f-4ce1-b580-c429de5324e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\user/.cache\\torch\\hub\\facebookresearch_pytorchvideo_master\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOW_8x8_R50.pyth\" to C:\\Users\\user/.cache\\torch\\hub\\checkpoints\\SLOW_8x8_R50.pyth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61ce70186684decaca42f358dfb8707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose the `slow_r50` pretrained model - for our video classification model training \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b3e29f-9109-4efd-a9bb-4dae93f4d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to GPU or CPU\n",
    "device = \"cpu\"\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9553cb5a-f2bc-4bc1-bbc1-41b60f80ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "json_filename = \"kinetics_classnames.json\"\n",
    "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "except: urllib.request.urlretrieve(json_url, json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e2a8a6-6198-4f3e-96ce-24e7a9797c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_filename, \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6d027-f76a-4bab-a084-dd3890d2fd5f",
   "metadata": {},
   "source": [
    "* Input Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba436b4-032f-4091-a960-942d82439135",
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 8\n",
    "sampling_rate = 8\n",
    "frames_per_second = 30\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f574d8-8854-4a21-a450-a0b861600692",
   "metadata": {},
   "source": [
    "* Loading video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b530da-f207-465e-9d03-1fe026c3c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
    "video_path = 'archery.mp4'\n",
    "try: urllib.URLopener().retrieve(url_link, video_path)\n",
    "except: urllib.request.urlretrieve(url_link, video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df94b3c-c1d8-4f41-9fe4-d59ba2849e1b",
   "metadata": {},
   "source": [
    "* Load the video and transform into input format (for the model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60431d9e-7340-4821-82dd-9a78eefe41f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\envs\\vc\\lib\\site-packages\\iopath\\common\\file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"C:\\Users\\user\\anaconda3\\envs\\vc\\lib\\site-packages\\iopath\\common\\event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n"
     ]
    }
   ],
   "source": [
    "# Select the duration of the clip to load by specifying the start and end duration\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "\n",
    "# Initialize an EncodedVideo helper class and load the video\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)\n",
    "\n",
    "# Move the inputs to the desired device\n",
    "inputs = video_data[\"video\"]\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6a356-4d20-47c8-890c-d463dae30c90",
   "metadata": {},
   "source": [
    "* Predictions on Video Clip. Output come with top 5 predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4295d9e6-3ba8-419f-be3c-c4fb90a85ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predicted labels: archery, throwing axe, playing paintball, stretching arm, riding or walking with horse\n"
     ]
    }
   ],
   "source": [
    "# Pass the input clip through the model\n",
    "preds = model(inputs[None, ...])\n",
    "\n",
    "# Get the predicted classes\n",
    "post_act = torch.nn.Softmax(dim=1)\n",
    "preds = post_act(preds)\n",
    "pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "# Map the predicted classes to the label names\n",
    "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c321ae3-74e7-4e4a-9133-fa2fe9e73a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "485cee6c-8ec0-4d67-9f9b-368eddbb534a",
   "metadata": {},
   "source": [
    "## 2. PyTorch - using pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54801f60-9c9e-443c-bbb0-51ff80cdff46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32144/1927808439.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import PIL\n",
    "import collections\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aed5c7-2cad-4d70-906f-c5cdebe89461",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size = 2, shuffle = True)\n",
    "\n",
    "dataset = datasets.VideoDataset(\n",
    "\t\"./data/example_video_file.csv\",\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        transforms.VideoFilePathToTensor(max_len=50, fps=10, padding_mode='last'),\n",
    "        transforms.VideoRandomCrop([512, 512]),\n",
    "        transforms.VideoResize([256, 256]),\n",
    "    ])\n",
    ")\n",
    "\n",
    "for videos in data_loader:\n",
    "    print(videos.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c9e58-5cd6-4294-a126-0e76dddbc6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775806a-ef7b-492d-a5cc-be0ba9a7abbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23689efe-9037-4d56-8519-2d4d496b05dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ae84f-4c4b-4fa8-98f8-3fbc0904d761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9c8d5-8c9d-4116-85c3-d4170d497c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a43ff9-ef0c-413f-baf9-0599a6bf5e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2868d0-2c95-453a-ba08-8c935243fe41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2875c-cb83-4d33-b3b2-b6b0c993933a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6388741e-cbec-438e-8b27-0315610f66a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ad5c2-7a7b-4001-b4b2-9892d25faa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124d243-7506-4e88-b54b-091d65f40afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c895a-1c47-42ea-80b0-2c3ee73b6c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d2c19-c0fc-4926-94ee-ef2b928e3ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032fe8b6-488c-4de9-9d78-877c4b36d973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712c09e-05f4-4ce4-a08e-3bc7821eec21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ba856-ffca-4ec1-b227-94b7e100111f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22d9c4-8b34-4d50-becb-4743403e2686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b028d5c-7de4-42d9-bc1b-8dfbc7237e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
