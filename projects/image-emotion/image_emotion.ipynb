{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ddf410b-5a3f-434f-a554-866ce61aa0e9",
   "metadata": {},
   "source": [
    "# Image-emotion project (from HCI course)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19edb57b-fc6b-40df-b2bd-4f57056e8f06",
   "metadata": {},
   "source": [
    "## Requirements import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7347482-3783-4573-b905-ffdeb4853b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required components\n",
    "import os,sys\n",
    "import glob, time, json\n",
    "import urllib\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606c8f7-14b2-4e43-9dc2-b51e422fea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imutils import paths\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "# from keras.models import Model\n",
    "# from keras.utils import np_utils\n",
    "from PIL import Image\n",
    "# from pdf2image import convert_from_path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b677aa-a90c-4cc4-a1bc-25c443dcd432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec995d22-0148-473a-8a3f-a0f466417e28",
   "metadata": {},
   "source": [
    "## Environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd875d-b832-4ea2-8237-83dd22762f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Setting torch environment\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     DEVICE = torch.device('cuda')\n",
    "# else:\n",
    "#     DEVICE = torch.device('cpu')\n",
    "\n",
    "# print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad5d8e-4c3a-429e-968f-e568212cac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f38f9-112f-4f45-a1fc-936d362ef3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # arugments\n",
    "    epochs=10\n",
    "    bs=16\n",
    "    lr=0.001\n",
    "    momentum=0.9\n",
    "    num_channels=3\n",
    "    num_classes=6\n",
    "    verbose='store_true'\n",
    "    seed=412\n",
    "\n",
    "args = Args()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b497f1-5a29-4b44-b889-5e3d6d69fdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710fe18d-9922-413e-822b-767a64ecb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18 = models.resnet18(pretrained=True).to(DEVICE)\n",
    "## resnet 구조는 마지막 fc layer의 out_features 를 바꿔주면 되고.\n",
    "model_resnet18.fc = nn.Linear(in_features = 512, out_features = args.num_classes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f6515-449c-4aea-ada8-919923779def",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_resnet18.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef68411-7bbc-4d3f-8229-247c47df39a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afc741-e873-468b-8b74-c4c3fa186a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((227,227)),\n",
    "    # transforms.Resize((256,256)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "#     transforms.ColorJitter(contrast=(0.3, 1), saturation=(0.3, 1)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d909919-9c31-4ded-b80c-5bfac8a6158a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f99a7d8d-e116-4918-aa99-c1150169b4ab",
   "metadata": {},
   "source": [
    "## Training data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810419f-8090-425c-b228-6575a3069332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading image data/content/drive/MyDrive/Research_personal/image_emotion/images\n",
    "emotion_data = torchvision.datasets.ImageFolder(root = 'E:/RESEARCH/Datasets/image_emotion/total/', transform = data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1203d-ae38-49f1-bfdc-991f3ef66473",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(emotion_data))\n",
    "test_size = len(emotion_data)-train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01093ed6-6e39-4648-ae7b-9cafb99b9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(emotion_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb2ade-f3c4-495c-b2bc-528478521547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.bs, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.bs, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfe9b6-5761-415a-84ff-230bf5165864",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c177e7-16dd-49be-a244-7de986ad5177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80795aac-7f7c-4545-8a1e-cabda02aaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Optimizer and Objective Function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() ## setup the loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** args.epochs)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=50,anneal_strategy='cos')\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3090b2-2975-4370-acf5-aa1cc6417b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9a940f-ccf3-4adf-a8c0-f1e9363eb226",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb56028-430b-4ff4-9eae-479c2376f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during CNN model\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image),\n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "\n",
    "#     scheduler.step() #for learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c2892-3889-4e0a-b558-911d4d46fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking model performance during the learning process\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    validation =[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "\n",
    "\n",
    "    test_loss /= (len(test_loader))\n",
    "    validation_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    validation.append(validation_accuracy)\n",
    "\n",
    "    return test_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79e2c4-9c23-40ef-9af2-ebdc6b422087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking train, val loss and accuracy\n",
    "\n",
    "los_total = []\n",
    "acc_total = []\n",
    "\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, validation_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tValidation Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, validation_accuracy))\n",
    "\n",
    "    los_total.append(test_loss)\n",
    "    acc_total.append(validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f30ee0-c21a-41d1-be41-60c7130601e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6af5985-9a7a-4859-8634-bb8beed9bdc0",
   "metadata": {},
   "source": [
    "## Save the trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58263714-5ee7-4d20-bc1a-1bc4d67adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이렇게 해야 완전하게 모델 저장이 됨. (weight 외의 항목들도)\n",
    "torch.save(model.state_dict(), 'E:/RESEARCH/Datasets/image_emotion/model_weights_six.pth')\n",
    "# torch.save(model.state_dict(), 'E:/RESEARCH/Datasets/image_emotion/model_weights_four.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e135f57-ec53-47b8-81fe-03e483476863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('E:/RESEARCH/Datasets/image_emotion/model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40f406-9965-4c7e-b45d-90a6c71d885c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbdfc070-e214-4cf9-84f2-521bf25739e6",
   "metadata": {},
   "source": [
    "## Inference check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab06b34-4617-4376-b5e6-c6ef92e18a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('anger', 'happiness', 'sadness', 'fear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da9c0c-06f4-4017-a05a-4ba065722ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Image.open('E:/RESEARCH/Datasets/image_emotion/sample1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9856865-ca13-46d7-9c8f-39cda65f0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_t = data_transforms(sample).to(DEVICE)\n",
    "batch_t = torch.unsqueeze(sample_t, 0)\n",
    "out = model(batch_t).to(DEVICE)\n",
    "model.eval()\n",
    "_, predicted = torch.sort(out, descending = True)\n",
    "print([(classes[idx]) for idx in predicted[0][:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897dfd0a-2e35-4249-b309-49712c25a10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56c9b8-d012-4ce2-ac45-389d636e85aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76fa908a-3fe7-4a33-ade2-6b167a0594b1",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85f7d3-2b8d-4f37-a2a9-f34ec93f20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_emotion(data, model):\n",
    "    ## get data\n",
    "    img = data\n",
    "    ## data transform for training\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227, 227)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img_t = transform(img).to(DEVICE)\n",
    "    ## resize to batch\n",
    "    batch_t = torch.unsqueeze(img_t, 0)\n",
    "    ## set the classes\n",
    "    classes = ('anger', 'fear', 'sadness', 'happiness')\n",
    "    ## model evaluate\n",
    "    model.eval() \n",
    "    ## inference\n",
    "    out = model(batch_t).to(DEVICE)\n",
    "    _, predicted = torch.sort(out, descending = True)\n",
    "    print(\"predicted emotion is:\", [(classes[idx]) for idx in predicted[0][:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df133fd8-bb20-471e-9e01-977f906d6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Image.open('E:/RESEARCH/Datasets/image_emotion/sample13.jpg')\n",
    "model_ = models.resnet18(pretrained=True).to(DEVICE)\n",
    "model_.fc = nn.Linear(in_features = 512, out_features = 4).to(DEVICE)\n",
    "model = model_.to(DEVICE)\n",
    "# model.load_state_dict(torch.load('D:/image_data/classify.pth'))\n",
    "model.load_state_dict(torch.load('E:/RESEARCH/Datasets/image_emotion/model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a54198-1713-4594-8294-9fde9cb359bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_emotion(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9870f0-8e3a-4e6a-ba03-ee927c7b7651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
